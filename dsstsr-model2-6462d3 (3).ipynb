{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12618365,"sourceType":"datasetVersion","datasetId":7972063}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gputil h5py timm PyWavelets -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:42:29.232655Z","iopub.execute_input":"2025-08-01T06:42:29.232890Z","iopub.status.idle":"2025-08-01T06:43:57.894390Z","shell.execute_reply.started":"2025-08-01T06:42:29.232867Z","shell.execute_reply":"2025-08-01T06:43:57.893706Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q timm --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:43:57.895297Z","iopub.execute_input":"2025-08-01T06:43:57.895513Z","iopub.status.idle":"2025-08-01T06:44:05.232095Z","shell.execute_reply.started":"2025-08-01T06:43:57.895478Z","shell.execute_reply":"2025-08-01T06:44:05.231204Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ‚úÖ Use a compatible torchvision version for timm\n!pip install -q timm==0.9.12 torchvision==0.15.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:44:05.233978Z","iopub.execute_input":"2025-08-01T06:44:05.234208Z","iopub.status.idle":"2025-08-01T06:46:08.874208Z","shell.execute_reply.started":"2025-08-01T06:44:05.234185Z","shell.execute_reply":"2025-08-01T06:46:08.873257Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.2 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom scipy.io import loadmat\nimport numpy as np\nimport cv2\nimport os\nimport timm\nfrom timm.models.swin_transformer import SwinTransformerBlock\nimport pywt\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport random\nimport h5py\nimport GPUtil\nfrom torch.cuda.amp import GradScaler\nfrom torch import amp\n# Suppress the UserWarning from PyWavelets for small patches\nwarnings.filterwarnings(\"ignore\", message=\"Level value of .* is too high: all coefficients will experience boundary effects.\")\n\nprint(\"‚úÖ Setup complete. All packages installed and libraries imported.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:08.875821Z","iopub.execute_input":"2025-08-01T06:46:08.876193Z","iopub.status.idle":"2025-08-01T06:46:19.616903Z","shell.execute_reply.started":"2025-08-01T06:46:08.876155Z","shell.execute_reply":"2025-08-01T06:46:19.616260Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Setup complete. All packages installed and libraries imported.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def create_lr_hr_pairs(hsi_cube, scale_factor=4):\n    \"\"\"Creates Low-Res (downsampled) and High-Res pairs for training.\"\"\"\n    hr_image = hsi_cube\n    blurred_hr = cv2.GaussianBlur(hr_image, (5, 5), 0)\n    lr_h, lr_w = hr_image.shape[0] // scale_factor, hr_image.shape[1] // scale_factor\n    lr_downsampled = cv2.resize(blurred_hr, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)\n    return lr_downsampled, hr_image\n\nclass WaveletDenoise3D(nn.Module):\n    \"\"\"Implements the 3D wavelet denoising method from the paper.\"\"\"\n    def __init__(self, wavelet='db4', level=1):\n        super().__init__()\n        self.wavelet = wavelet\n        self.level = level\n        self.coeffs_to_remove = ['aad', 'ada', 'daa', 'ddd']\n\n    def forward(self, x):\n        input_numpy = x.detach().cpu().numpy()\n        output_numpy = np.empty_like(input_numpy)\n        for i in range(input_numpy.shape[0]):\n            volume = input_numpy[i]\n            coeffs = pywt.wavedecn(volume, self.wavelet, level=self.level)\n            for detail_dict in coeffs[1:]:\n                for key in self.coeffs_to_remove:\n                    if key in detail_dict:\n                        detail_dict[key] = np.zeros_like(detail_dict[key])\n            denoised_volume = pywt.waverecn(coeffs, self.wavelet)\n            c, h, w = volume.shape\n            output_numpy[i] = denoised_volume[:c, :h, :w]\n        return torch.from_numpy(output_numpy).to(x.device)\n\ndef denoise_cube(lr_cube):\n    \"\"\"Applies the 3D wavelet denoising to an entire HSI cube one time before training.\"\"\"\n    print(\"Starting one-time denoising of the LR cube...\")\n    temp_tensor = torch.from_numpy(lr_cube).permute(2, 0, 1).unsqueeze(0)\n    denoiser = WaveletDenoise3D()\n    denoised_tensor = denoiser(temp_tensor)\n    denoised_cube = denoised_tensor.squeeze(0).permute(1, 2, 0).numpy()\n    print(\"Denoising complete.\")\n    return denoised_cube\n\nprint(\"‚úÖ Data preparation functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:19.617628Z","iopub.execute_input":"2025-08-01T06:46:19.617985Z","iopub.status.idle":"2025-08-01T06:46:19.627044Z","shell.execute_reply.started":"2025-08-01T06:46:19.617967Z","shell.execute_reply":"2025-08-01T06:46:19.626375Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data preparation functions defined.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport cv2\n\ndef load_scene_from_pngs(scene_path):\n    \"\"\"Load a 31-band HSI scene from sorted PNGs in a directory.\"\"\"\n    # ‚ö†Ô∏è Only include files that match the *_ms_XX.png format\n    png_files = [\n        f for f in os.listdir(scene_path)\n        if f.endswith('.png') and '_ms_' in f\n    ]\n    \n    # ‚úÖ Sort by band number extracted from filename\n    def band_number(filename):\n        parts = filename.split('_')\n        band_str = parts[-1].replace('.png', '')\n        return int(band_str)\n\n    png_files = sorted(png_files, key=band_number)\n\n    if len(png_files) != 31:\n        print(f\"    ‚ùå Skipping: Found {len(png_files)} bands, expected 31.\")\n        return None\n\n    first_img_path = os.path.join(scene_path, png_files[0])\n    sample_img = cv2.imread(first_img_path, cv2.IMREAD_GRAYSCALE)\n    if sample_img is None:\n        print(f\"    ‚ùå Corrupt image: {png_files[0]}\")\n        return None\n    \n    h, w = sample_img.shape\n    hsi_cube = np.zeros((h, w, 31), dtype=np.float32)\n\n    for i, fname in enumerate(png_files):\n        file_path = os.path.join(scene_path, fname)\n        band_img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n        if band_img is None:\n            print(f\"    ‚ö†Ô∏è Skipping corrupt image: {file_path}\")\n            return None\n        # ‚úÖ Normalize 8-bit image\n        hsi_cube[:, :, i] = band_img.astype(np.float32) / 255.0\n\n    return hsi_cube\n\n# Dummy placeholders ‚Äì replace with your actual functions\ndef create_lr_hr_pairs(hsi_cube, scale_factor):\n    hr = hsi_cube[np.newaxis, ...]  # Add batch dimension\n    lr = cv2.resize(hsi_cube, (hsi_cube.shape[1] // scale_factor, hsi_cube.shape[0] // scale_factor))\n    lr = cv2.resize(lr, (hsi_cube.shape[1], hsi_cube.shape[0]))  # Upsample back\n    lr = lr[np.newaxis, ...]\n    return lr, hr\n\ndef denoise_cube(cube):\n    return cube  # Placeholder (no denoising)\n\ndef prepare_data_cubes_from_folders(scene_folder_list, scale_factor=4):\n    hr_cubes, lr_denoised_cubes = [], []\n\n    for scene_path in scene_folder_list:\n        print(f\"  Processing: {os.path.basename(scene_path)}\")\n        hsi_cube = load_scene_from_pngs(scene_path)\n\n        if hsi_cube is None:\n            continue\n\n        # Crop to multiple of scale\n        h, w, c = hsi_cube.shape\n        hsi_cube = hsi_cube[:h - (h % scale_factor), :w - (w % scale_factor), :]\n\n        # Normalize each band (again, optional ‚Äî already normalized)\n        for i in range(c):\n            band = hsi_cube[:, :, i]\n            min_val, max_val = np.min(band), np.max(band)\n            if max_val > min_val:\n                hsi_cube[:, :, i] = (band - min_val) / (max_val - min_val)\n\n        lr_down, hr = create_lr_hr_pairs(hsi_cube, scale_factor=scale_factor)\n        lr_denoised = denoise_cube(lr_down)\n\n        hr_cubes.append(hr)\n        lr_denoised_cubes.append(lr_denoised)\n\n    if not hr_cubes:\n        raise ValueError(\"‚ùå No valid 31-band scenes found.\")\n\n    return np.concatenate(hr_cubes, axis=0), np.concatenate(lr_denoised_cubes, axis=0)\n\n# --- Traverse the double-nested dataset structure ---\ndataset_path = '/kaggle/input/cave-hsi/'\n\nscene_folders = []\n\nfor scene_name in os.listdir(dataset_path):\n    outer_path = os.path.join(dataset_path, scene_name)\n    inner_path = os.path.join(outer_path, scene_name)\n    \n    if os.path.isdir(inner_path):\n        png_files = [f for f in os.listdir(inner_path) if f.endswith('.png') and '_ms_' in f]\n        if len(png_files) == 31:\n            scene_folders.append(inner_path)\n        else:\n            print(f\"‚ö†Ô∏è Skipping {scene_name}: found {len(png_files)} PNGs, expected 31.\")\n\nprint(f\"‚úÖ Ready to process {len(scene_folders)} valid scenes.\")\nhr_cubes, lr_denoised_cubes = prepare_data_cubes_from_folders(scene_folders)\nprint(f\"‚úÖ HR shape: {hr_cubes.shape}, LR shape: {lr_denoised_cubes.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:19.627918Z","iopub.execute_input":"2025-08-01T06:46:19.628171Z","iopub.status.idle":"2025-08-01T06:46:51.949357Z","shell.execute_reply.started":"2025-08-01T06:46:19.628149Z","shell.execute_reply":"2025-08-01T06:46:51.948546Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Ready to process 32 valid scenes.\n  Processing: oil_painting_ms\n  Processing: superballs_ms\n  Processing: egyptian_statue_ms\n  Processing: fake_and_real_tomatoes_ms\n  Processing: photo_and_face_ms\n  Processing: glass_tiles_ms\n  Processing: beads_ms\n  Processing: fake_and_real_lemon_slices_ms\n  Processing: hairs_ms\n  Processing: chart_and_stuffed_toy_ms\n  Processing: watercolors_ms\n  Processing: clay_ms\n  Processing: stuffed_toys_ms\n  Processing: fake_and_real_peppers_ms\n  Processing: fake_and_real_strawberries_ms\n  Processing: sponges_ms\n  Processing: face_ms\n  Processing: cd_ms\n  Processing: fake_and_real_beers_ms\n  Processing: real_and_fake_apples_ms\n  Processing: feathers_ms\n  Processing: fake_and_real_food_ms\n  Processing: jelly_beans_ms\n  Processing: balloons_ms\n  Processing: thread_spools_ms\n  Processing: flowers_ms\n  Processing: paints_ms\n  Processing: pompoms_ms\n  Processing: fake_and_real_sushi_ms\n  Processing: cloth_ms\n  Processing: real_and_fake_peppers_ms\n  Processing: fake_and_real_lemons_ms\n‚úÖ HR shape: (32, 512, 512, 31), LR shape: (32, 512, 512, 31)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class HSISuperResolutionDataset(Dataset):\n    def __init__(self, hr_cube, lr_downsampled_cube, hr_patch_size=63, scale_factor=4):\n        self.hr_cube = hr_cube\n        self.lr_cube = lr_downsampled_cube\n        self.hr_patch_size = hr_patch_size\n        self.lr_patch_size = hr_patch_size // scale_factor\n        self.scale_factor = scale_factor\n\n        self.n_scenes, self.h, self.w, self.bands = self.hr_cube.shape\n        self.patch_step = self.hr_patch_size // 2\n        self.band_groups = self.bands // 5\n        self.patch_coords = [\n            (scene_idx, r, c)\n            for scene_idx in range(self.n_scenes)\n            for r in range(0, self.h - self.hr_patch_size + 1, self.patch_step)\n            for c in range(0, self.w - self.hr_patch_size + 1, self.patch_step)\n        ]\n        self.total_samples = len(self.patch_coords) * self.band_groups\n\n    def __len__(self):\n        return self.total_samples\n\n    def __getitem__(self, index):\n        patch_index = index % len(self.patch_coords)\n        band_group_index = index // len(self.patch_coords)\n        scene_idx, r, c = self.patch_coords[patch_index]\n        hr_patch = self.hr_cube[scene_idx, r:r + self.hr_patch_size, c:c + self.hr_patch_size, :]\n        lr_patch = self.lr_cube[scene_idx,\n                                r // self.scale_factor:r // self.scale_factor + self.lr_patch_size,\n                                c // self.scale_factor:c // self.scale_factor + self.lr_patch_size, :]\n        start_band = band_group_index * 5\n        end_band = start_band + 5\n        hr_patch = hr_patch[:, :, start_band:end_band]\n        lr_patch = lr_patch[:, :, start_band:end_band]\n\n        if np.random.rand() > 0.5:\n            lr_patch = np.ascontiguousarray(np.flip(lr_patch, axis=1))\n            hr_patch = np.ascontiguousarray(np.flip(hr_patch, axis=1))\n        if np.random.rand() > 0.5:\n            lr_patch = np.ascontiguousarray(np.flip(lr_patch, axis=0))\n            hr_patch = np.ascontiguousarray(np.flip(hr_patch, axis=0))\n\n        lr_tensor = torch.from_numpy(lr_patch.copy()).float().permute(2, 0, 1)\n        center_band_idx = hr_patch.shape[2] // 2\n        hr_tensor = torch.from_numpy(hr_patch[:, :, center_band_idx:center_band_idx+1].copy()).float().permute(2, 0, 1)\n        return lr_tensor, hr_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:51.950194Z","iopub.execute_input":"2025-08-01T06:46:51.950418Z","iopub.status.idle":"2025-08-01T06:46:51.959390Z","shell.execute_reply.started":"2025-08-01T06:46:51.950389Z","shell.execute_reply":"2025-08-01T06:46:51.958863Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ShallowFeatureExtractor(nn.Module):\n    def __init__(self, in_channels=5, base_channels=64):\n        super().__init__()\n        self.conv2d_initial = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        self.res_block = nn.Sequential(\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1)\n        )\n        self.spatial_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(3, 3, 3), padding=1)\n        self.spectral_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(3, 3, 3), padding=1)\n        self.fusion_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(1, 1, 1))\n\n    def forward(self, x):\n        t = self.conv2d_initial(x)\n        t = t + self.res_block(t)\n        t_3d = t.unsqueeze(2)\n        feat_spa = self.spatial_conv3d(t_3d)\n        feat_spec = self.spectral_conv3d(t_3d)\n        fused_feat = feat_spa + feat_spec\n        return self.fusion_conv3d(fused_feat).squeeze(2)\n\nclass SwinTransformerBlockWrapper(nn.Module):\n    def __init__(self, dim, num_heads, window_size=7):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n\n    def forward(self, x):\n        B, N, C = x.shape\n        H = W = int(np.sqrt(N))\n        swin_block = SwinTransformerBlock(\n            dim=self.dim,\n            input_resolution=(H, W),\n            num_heads=self.num_heads,\n            window_size=7,\n            shift_size=3\n        ).to(x.device)\n        x = x.view(B, H, W, C)\n        x = swin_block(x)\n        return x.view(B, -1, C)\n\nclass SpectralAttention(nn.Module):\n    def __init__(self, dim, num_heads=8):\n        super().__init__()\n        assert dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.proj = nn.Linear(dim, dim)\n        self.sigma = nn.Parameter(torch.ones(1))  # Learnable\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (self.sigma * q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        return self.proj(x)\nclass DSAL(nn.Module):\n    def __init__(self, dim, num_heads, window_size=7):\n        super().__init__()\n        self.norm1, self.norm2 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n        self.norm3, self.norm4 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n        self.spatial_attention = SwinTransformerBlockWrapper(dim, num_heads, window_size=7)\n        self.spectral_attention = SpectralAttention(dim, num_heads)\n        self.mlp1 = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n        self.mlp2 = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n\n    def forward(self, x):\n        x = x + self.spatial_attention(self.norm1(x))\n        x = x + self.mlp1(self.norm2(x))\n        x = x + self.spectral_attention(self.norm3(x))\n        x = x + self.mlp2(self.norm4(x))\n        return x\n\nclass DSST_Module(nn.Module):\n    def __init__(self, dim, num_heads, num_dsal=6, window_size=7):\n        super().__init__()\n        self.dsal_blocks = nn.ModuleList([DSAL(dim, num_heads, window_size=7) for _ in range(num_dsal)])\n        self.conv_final = nn.Conv2d(dim, dim, 3, padding=1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x_res = x\n        x = x.flatten(2).transpose(1, 2)\n        for block in self.dsal_blocks:\n            x = block(x)\n        x = x.transpose(1, 2).reshape(B, C, H, W)\n        return self.conv_final(x) + x_res\n        \nclass DSSTSR(nn.Module):\n    def __init__(self, in_channels=5, base_channels=64, out_channels=1, scale_factor=4, num_dsst_modules=4, window_size=7):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.shallow_extractor = ShallowFeatureExtractor(in_channels=in_channels, base_channels=base_channels)\n        self.deep_extractor = nn.Sequential(*[DSST_Module(base_channels, num_heads=8, window_size=window_size)\n                                              for _ in range(num_dsst_modules)])\n        self.upsampler = nn.Sequential(\n            nn.Conv2d(base_channels, out_channels * (scale_factor ** 2), 3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n\n    def forward(self, x):\n        center_band_idx = x.shape[1] // 2\n        bicubic_upsampled_input = F.interpolate(\n            x[:, center_band_idx:center_band_idx+1, :, :], \n            scale_factor=self.scale_factor, \n            mode='bicubic', \n            align_corners=False\n        )\n        shallow_features = self.shallow_extractor(x)\n        deep_features = self.deep_extractor(shallow_features)\n        residual = self.upsampler(deep_features)\n        return bicubic_upsampled_input + residual\n\nprint(\"‚úÖ Model architecture classes defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:51.960288Z","iopub.execute_input":"2025-08-01T06:46:51.960588Z","iopub.status.idle":"2025-08-01T06:46:51.993002Z","shell.execute_reply.started":"2025-08-01T06:46:51.960563Z","shell.execute_reply":"2025-08-01T06:46:51.992453Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model architecture classes defined.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class HSI_Classifier(nn.Module):\n    def __init__(self, num_bands, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(num_bands, 100)\n        self.fc2 = nn.Linear(100, num_classes)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\nclass PixelDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = torch.from_numpy(X).float(), torch.from_numpy(y).long()\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ndef run_classification_evaluation(sr_image_cube, gt_path):\n    print(\"\\n--- Starting Classification Evaluation ---\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load ground truth\n    gt_data = loadmat(gt_path)\n    gt_map = gt_data['indian_pines_gt']\n    \n    # --- ADD THIS CODE TO CROP THE GROUND TRUTH MAP ---\n    # Ensure gt_map has the same spatial dimensions as the sr_image_cube\n    h, w, _ = sr_image_cube.shape\n    gt_map = gt_map[:h, :w]\n    # --- End of New Code ---\n    \n    # Prepare pixel data (this line will now work correctly)\n    X_pixels, y_pixels = sr_image_cube[gt_map > 0], gt_map[gt_map > 0] - 1 \n    num_classes = len(np.unique(y_pixels))\n    \n    # Paper's \"30 samples per class\" strategy\n    train_size = 30 * num_classes\n    if train_size >= len(y_pixels): # Make sure train_size is not too large\n        train_size = int(len(y_pixels) * 0.5)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_pixels, y_pixels, train_size=train_size, stratify=y_pixels, random_state=42\n    )\n    \n    train_loader = DataLoader(PixelDataset(X_train, y_train), batch_size=32, shuffle=True)\n    test_loader = DataLoader(PixelDataset(X_test, y_test), batch_size=128, shuffle=False)\n    \n    classifier = HSI_Classifier(num_bands=sr_image_cube.shape[2], num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n    \n    print(\"Training classifier...\")\n    for epoch in range(100): # Train for a reasonable number of epochs\n        for data, labels in train_loader:\n            data, labels = data.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = classifier(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n    \n    print(\"Evaluating classifier...\")\n    classifier.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for data, labels in test_loader:\n            data = data.to(device)\n            outputs = classifier(data)\n            _, predicted = torch.max(outputs.data, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    oa = accuracy_score(all_labels, all_preds)\n    kappa = cohen_kappa_score(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n    aa = np.mean([report[str(i)]['recall'] for i in range(num_classes)])\n    \n    return {'OA': oa, 'AA': aa, 'Kappa': kappa}\nprint(\"‚úÖ Classification experiment functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:51.995148Z","iopub.execute_input":"2025-08-01T06:46:51.995612Z","iopub.status.idle":"2025-08-01T06:46:52.017315Z","shell.execute_reply.started":"2025-08-01T06:46:51.995593Z","shell.execute_reply":"2025-08-01T06:46:52.016770Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Classification experiment functions defined.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import time\ndef init_weights(m):\n    \"\"\"Applies Kaiming initialization to conv and linear layers.\"\"\"\n    if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.Linear)):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n            \ndef compute_metrics(y_true, y_pred):\n    y_true, y_pred = np.clip(y_true, 0, 1), np.clip(y_pred, 0, 1)\n\n    psnr_val = psnr(y_true, y_pred, data_range=1.0)\n\n    ssim_val = ssim(\n        y_true, y_pred, data_range=1.0, multichannel=True, channel_axis=-1, win_size=11\n    )\n\n    y_true_flat = y_true.reshape(-1, y_true.shape[-1])\n    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n\n    dot_products = np.sum(y_true_flat * y_pred_flat, axis=1)\n    norms_true = np.linalg.norm(y_true_flat, axis=1)\n    norms_pred = np.linalg.norm(y_pred_flat, axis=1)\n\n    valid_mask = (norms_true > 1e-10) & (norms_pred > 1e-10)\n    sam_values = np.zeros(len(dot_products))\n\n    if np.any(valid_mask):\n        cos_angles = dot_products[valid_mask] / (norms_true[valid_mask] * norms_pred[valid_mask])\n        cos_angles = np.clip(cos_angles, -1, 1)\n        sam_values[valid_mask] = np.arccos(cos_angles)\n\n    sam_mean = np.mean(sam_values) * 180 / np.pi\n    return {'PSNR': psnr_val, 'SSIM': ssim_val, 'SAM': sam_mean}\n\n\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef init_weights(m):\n    \"\"\"Applies Kaiming initialization to conv and linear layers.\"\"\"\n    if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.Linear)):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\ndef run_experiment_fixed(hr_train_cube, lr_train_denoised_cube, test_files, num_epochs=175, scale_factor=4, hr_patch_size=63):\n    import time, os, cv2\n    import torch\n    import numpy as np\n    from torch.cuda.amp import autocast, GradScaler\n    from torch.utils.data import DataLoader\n    def get_lr_with_warmup(epoch, base_lr=1e-4, warmup_epochs=5):\n        if epoch < warmup_epochs:\n            return base_lr * (epoch + 1) / warmup_epochs\n        else:\n            return base_lr * (0.5 ** ((epoch - warmup_epochs) // 35))\n\n\n    print(\"\\n--- Starting Super-Resolution Experiment (Efficient Version) ---\")\n    lr_patch_size = hr_patch_size // scale_factor\n\n    # Prepare validation data\n    print(\"Preparing validation data from the first test scene...\")\n    hr_val_scene, lr_val_denoised_scene = prepare_data_cubes_from_folders([test_files[0]], scale_factor=scale_factor)\n\n    train_dataset = HSISuperResolutionDataset(hr_train_cube, lr_train_denoised_cube, hr_patch_size, scale_factor)\n    val_dataset = HSISuperResolutionDataset(hr_val_scene, lr_val_denoised_scene, hr_patch_size, scale_factor)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def paper_faithful_init(m):\n        \n        if isinstance(m, (nn.Conv2d, nn.Conv3d)):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    model = DSSTSR(in_channels=5, out_channels=1, base_channels=64, scale_factor=scale_factor, window_size=7)\n    model.apply(paper_faithful_init)\n\n\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Activating data parallelism on {torch.cuda.device_count()} GPUs!\")\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n\n    criterion = torch.nn.L1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    \n    def lr_lambda(epoch):\n        return 0.5 ** (epoch // 35)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = GradScaler()\n\n    best_val_loss = float('inf')\n    print(\"Starting training...\")\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_loss = 0.0\n        epoch_start_time = time.time()\n\n        for i, (lr_batch, hr_batch) in enumerate(train_loader):\n            lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n            optimizer.zero_grad()\n            with autocast():\n                preds = model(lr_batch)\n                min_h = min(preds.shape[2], hr_batch.shape[2])\n                min_w = min(preds.shape[3], hr_batch.shape[3])\n                preds = preds[:, :, :min_h, :min_w]\n                hr_batch = hr_batch[:, :, :min_h, :min_w]\n                loss = criterion(preds, hr_batch)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            train_loss += loss.item()\n\n            if i % 10 == 0:\n                print(f\"  [Epoch {epoch}/{num_epochs}] Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n\n        avg_train_loss = train_loss / len(train_loader)\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for lr_batch, hr_batch in val_loader:\n                lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n                with autocast():\n                    preds = model(lr_batch)\n                    min_h = min(preds.shape[2], hr_batch.shape[2])\n                    min_w = min(preds.shape[3], hr_batch.shape[3])\n                    preds = preds[:, :, :min_h, :min_w]\n                    hr_batch = hr_batch[:, :, :min_h, :min_w]\n                    loss = criterion(preds, hr_batch)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        scheduler.step()\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), 'best_dsstsr_model_fixed.pth')\n\n        if epoch % 10 == 0:\n            print(f\"\\n[Epoch {epoch}/{num_epochs}] ‚úÖ Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}, Time: {time.time() - epoch_start_time:.2f}s\")\n\n    print(\"\\n--- Training Complete. Loading best model for evaluation... ---\")\n    eval_model = DSSTSR(in_channels=5, out_channels=1, base_channels=64, scale_factor=scale_factor, window_size=7)\n    state_dict = torch.load('best_dsstsr_model_fixed.pth')\n    if any(k.startswith('module.') for k in state_dict.keys()):\n        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    eval_model.load_state_dict(state_dict)\n    eval_model.to(device)\n    eval_model.eval()\n\n    all_psnr, all_ssim, all_sam = [], [], []\n\n    print(f\"\\n--- Starting evaluation on {len(test_files)} test scenes ---\")\n\n    for scene_idx, test_file_path in enumerate(test_files):\n        scene_name = os.path.basename(test_file_path)\n        print(f\"\\n[{scene_idx+1}/{len(test_files)}] Evaluating scene: {scene_name}\")\n        \n        hr_test_scene, lr_test_denoised_scene = prepare_data_cubes_from_folders([test_file_path], scale_factor=scale_factor)\n        if hr_test_scene.ndim == 4: hr_test_scene = hr_test_scene[0]\n        if lr_test_denoised_scene.ndim == 4: lr_test_denoised_scene = lr_test_denoised_scene[0]\n        print(f\"    Scene shape: HR {hr_test_scene.shape}, LR {lr_test_denoised_scene.shape}\")\n\n        reconstructed_hr_cube = np.zeros_like(hr_test_scene)\n        total_bands = hr_test_scene.shape[2]\n\n        with torch.no_grad():\n            for band_idx in range(total_bands):\n                if band_idx % 5 == 0:\n                    print(f\"    Processing bands {band_idx+1}-{min(band_idx+5, total_bands)}...\")\n\n                L = hr_test_scene.shape[2]\n                if band_idx < 2:  # First 2 bands (0, 1)\n                    band_indices = [0, 1, 2, 3, 4]\n                elif band_idx >= L - 2:  # Last 2 bands (corrected logic)\n                    band_indices = [L-5, L-4, L-3, L-2, L-1]\n                else:  # Middle bands  \n                    band_indices = [band_idx-2, band_idx-1, band_idx, band_idx+1, band_idx+2]\n\n                lr_input_5_bands = lr_test_denoised_scene[:, :, band_indices]\n                h, w = lr_input_5_bands.shape[:2]\n                expected_hr_h, expected_hr_w = hr_test_scene.shape[:2]\n                patch_size = 63\n                step_size = 56\n\n                patches_h = (h - patch_size) // step_size + 1\n                patches_w = (w - patch_size) // step_size + 1\n\n                reconstructed_band = np.zeros((expected_hr_h, expected_hr_w), dtype=np.float32)\n                weight_map = np.zeros((expected_hr_h, expected_hr_w), dtype=np.float32)\n\n                for i in range(patches_h):\n                    for j in range(patches_w):\n                        r_start = i * step_size\n                        c_start = j * step_size\n                        r_end = min(r_start + patch_size, h)\n                        c_end = min(c_start + patch_size, w)\n\n                        lr_patch = lr_input_5_bands[r_start:r_end, c_start:c_end, :]\n                        actual_h, actual_w = lr_patch.shape[:2]\n                        if actual_h < patch_size or actual_w < patch_size:\n                            padded_patch = np.zeros((patch_size, patch_size, 5), dtype=np.float32)\n                            padded_patch[:actual_h, :actual_w, :] = lr_patch\n                            lr_patch = padded_patch\n\n                        lr_tensor = torch.from_numpy(lr_patch).float().permute(2, 0, 1).unsqueeze(0).to(device)\n                        try:\n                            hr_patch_tensor = eval_model(lr_tensor)\n                            hr_patch = hr_patch_tensor.squeeze(0).squeeze(0).cpu().numpy()\n\n                            hr_r_start = r_start * scale_factor\n                            hr_c_start = c_start * scale_factor\n                            hr_r_end = min(hr_r_start + actual_h * scale_factor, expected_hr_h)\n                            hr_c_end = min(hr_c_start + actual_w * scale_factor, expected_hr_w)\n\n                            out_h = hr_r_end - hr_r_start\n                            out_w = hr_c_end - hr_c_start\n                            if out_h > 0 and out_w > 0:\n                                hr_patch_crop = hr_patch[:out_h, :out_w]\n                                reconstructed_band[hr_r_start:hr_r_end, hr_c_start:hr_c_end] += hr_patch_crop\n                                weight_map[hr_r_start:hr_r_end, hr_c_start:hr_c_end] += 1.0\n                        except Exception as e:\n                            if (i * patches_w + j) % 50 == 0:\n                                print(f\"    Warning: Patch ({r_start},{c_start}) failed: {str(e)[:50]}...\")\n\n                weight_map = np.maximum(weight_map, 1.0)\n                reconstructed_band /= weight_map\n\n                if np.any(weight_map == 1.0):\n                    center_band = lr_input_5_bands[:, :, 2]\n                    bicubic_fallback = cv2.resize(center_band, (expected_hr_w, expected_hr_h), interpolation=cv2.INTER_CUBIC)\n                    reconstructed_band[weight_map == 1.0] = bicubic_fallback[weight_map == 1.0]\n\n                reconstructed_hr_cube[:, :, band_idx] = reconstructed_band\n\n        print(f\"    Computing metrics for {scene_name}...\")\n        scene_metrics = compute_metrics(hr_test_scene, reconstructed_hr_cube)\n        all_psnr.append(scene_metrics['PSNR'])\n        all_ssim.append(scene_metrics['SSIM'])\n        all_sam.append(scene_metrics['SAM'])\n        print(f\"    ‚úÖ {scene_name} - PSNR: {scene_metrics['PSNR']:.2f}, SSIM: {scene_metrics['SSIM']:.4f}, SAM: {scene_metrics['SAM']:.2f}\")\n\n    final_metrics = {\n        'PSNR': np.mean(all_psnr),\n        'SSIM': np.mean(all_ssim),\n        'SAM': np.mean(all_sam)\n    }\n\n    print(f\"\\nüéâ Evaluation Complete!\")\n    return final_metrics, reconstructed_hr_cube\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:52.018171Z","iopub.execute_input":"2025-08-01T06:46:52.018389Z","iopub.status.idle":"2025-08-01T06:46:52.049941Z","shell.execute_reply.started":"2025-08-01T06:46:52.018371Z","shell.execute_reply":"2025-08-01T06:46:52.049389Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\ndataset_path = '/kaggle/input/cave-hsi/'\n\nprint(f\"--- Attempting to list contents of: {dataset_path} ---\")\ntry:\n    top_level_contents = os.listdir(dataset_path)\n    print(\"Top-level contents are:\")\n    print(top_level_contents)\n\n    if top_level_contents:\n        # Check the first item in the list\n        first_item_name = top_level_contents[0]\n        first_item_path = os.path.join(dataset_path, first_item_name)\n        \n        if os.path.isdir(first_item_path):\n            print(f\"\\n--- Contents of the first folder '{first_item_name}' ---\")\n            print(os.listdir(first_item_path))\n        else:\n            print(f\"\\n'{first_item_name}' is a file, not a folder.\")\n\nexcept FileNotFoundError:\n    print(f\"‚ùå CRITICAL ERROR: The path '{dataset_path}' does not exist. Please check your dataset name.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:52.050648Z","iopub.execute_input":"2025-08-01T06:46:52.050875Z","iopub.status.idle":"2025-08-01T06:46:52.077767Z","shell.execute_reply.started":"2025-08-01T06:46:52.050853Z","shell.execute_reply":"2025-08-01T06:46:52.077100Z"}},"outputs":[{"name":"stdout","text":"--- Attempting to list contents of: /kaggle/input/cave-hsi/ ---\nTop-level contents are:\n['oil_painting_ms', 'superballs_ms', 'egyptian_statue_ms', 'fake_and_real_tomatoes_ms', 'photo_and_face_ms', 'glass_tiles_ms', 'beads_ms', 'fake_and_real_lemon_slices_ms', 'hairs_ms', 'chart_and_stuffed_toy_ms', 'watercolors_ms', 'clay_ms', 'stuffed_toys_ms', 'fake_and_real_peppers_ms', 'fake_and_real_strawberries_ms', 'sponges_ms', 'face_ms', 'cd_ms', 'fake_and_real_beers_ms', 'real_and_fake_apples_ms', 'feathers_ms', 'fake_and_real_food_ms', 'jelly_beans_ms', 'balloons_ms', 'thread_spools_ms', 'flowers_ms', 'paints_ms', 'pompoms_ms', 'fake_and_real_sushi_ms', 'cloth_ms', 'real_and_fake_peppers_ms', 'fake_and_real_lemons_ms']\n\n--- Contents of the first folder 'oil_painting_ms' ---\n['oil_painting_ms']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\n\n# --- 1. Define File Path and Get List of All Valid Scene Folders ---\ndataset_path = '/kaggle/input/cave-hsi'\nall_scene_folders = []\n\nfor scene_name in os.listdir(dataset_path):\n    outer_path = os.path.join(dataset_path, scene_name)\n    inner_path = os.path.join(outer_path, scene_name)\n\n    if os.path.isdir(inner_path):\n        png_files = [f for f in os.listdir(inner_path) if f.endswith('.png') and '_ms_' in f]\n        if len(png_files) == 31:\n            all_scene_folders.append(inner_path)\n\nrandom.shuffle(all_scene_folders)\ntrain_folders = all_scene_folders[:20]\ntest_folders = all_scene_folders[20:]\n\nprint(f\"üìÅ Dataset split: {len(train_folders)} training scenes, {len(test_folders)} testing scenes.\")\n\n# --- 2. Prepare Data Cubes ---\nprint(\"\\nüîß Preparing Training Data\")\nhr_train_cube, lr_train_denoised_cube = prepare_data_cubes_from_folders(train_folders)\nprint(f\"‚úÖ Final training HR shape: {hr_train_cube.shape}, LR shape: {lr_train_denoised_cube.shape}\")\n\nprint(\"\\nüîß Preparing Testing Data\")\nhr_test_cube, lr_test_denoised_cube = prepare_data_cubes_from_folders(test_folders)\nprint(f\"‚úÖ Final testing HR shape: {hr_test_cube.shape}, LR shape: {lr_test_denoised_cube.shape}\")\n\n# --- 3. Run Super-Resolution Experiment ---\nprint(\"\\nüöÄ Running Super-Resolution (Fixed DSSTSR Model)\")\nsr_metrics, reconstructed_hr_cube = run_experiment_fixed(\n    hr_train_cube=hr_train_cube,\n    lr_train_denoised_cube=lr_train_denoised_cube,\n    test_files=test_folders[:1],\n    num_epochs=1,         # Use 1 for quick testing\n    hr_patch_size=63      # Must be divisible by 7\n)\n\n# --- 4. Print SR Results ---\nprint(\"\\nüìä ‚úÖ Final Super-Resolution Results:\")\nfor metric, value in sr_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T06:46:52.078409Z","iopub.execute_input":"2025-08-01T06:46:52.078656Z"}},"outputs":[{"name":"stdout","text":"üìÅ Dataset split: 20 training scenes, 12 testing scenes.\n\nüîß Preparing Training Data\n  Processing: fake_and_real_lemon_slices_ms\n  Processing: clay_ms\n  Processing: photo_and_face_ms\n  Processing: fake_and_real_peppers_ms\n  Processing: jelly_beans_ms\n  Processing: hairs_ms\n  Processing: superballs_ms\n  Processing: fake_and_real_beers_ms\n  Processing: face_ms\n  Processing: paints_ms\n  Processing: fake_and_real_lemons_ms\n  Processing: real_and_fake_apples_ms\n  Processing: chart_and_stuffed_toy_ms\n  Processing: fake_and_real_food_ms\n  Processing: egyptian_statue_ms\n  Processing: flowers_ms\n  Processing: sponges_ms\n  Processing: fake_and_real_strawberries_ms\n  Processing: glass_tiles_ms\n  Processing: beads_ms\n‚úÖ Final training HR shape: (20, 512, 512, 31), LR shape: (20, 512, 512, 31)\n\nüîß Preparing Testing Data\n  Processing: real_and_fake_peppers_ms\n  Processing: feathers_ms\n  Processing: cd_ms\n  Processing: balloons_ms\n  Processing: oil_painting_ms\n  Processing: pompoms_ms\n  Processing: thread_spools_ms\n  Processing: watercolors_ms\n  Processing: fake_and_real_sushi_ms\n  Processing: stuffed_toys_ms\n  Processing: fake_and_real_tomatoes_ms\n  Processing: cloth_ms\n‚úÖ Final testing HR shape: (12, 512, 512, 31), LR shape: (12, 512, 512, 31)\n\nüöÄ Running Super-Resolution (Fixed DSSTSR Model)\n\n--- Starting Super-Resolution Experiment (Efficient Version) ---\nPreparing validation data from the first test scene...\n  Processing: real_and_fake_peppers_ms\nActivating data parallelism on 2 GPUs!\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"  [Epoch 1/1] Batch 1/844 - Loss: 158.1095\n  [Epoch 1/1] Batch 11/844 - Loss: 115.8140\n  [Epoch 1/1] Batch 21/844 - Loss: 80.0038\n  [Epoch 1/1] Batch 31/844 - Loss: 59.5807\n  [Epoch 1/1] Batch 41/844 - Loss: 48.0689\n  [Epoch 1/1] Batch 51/844 - Loss: 43.3678\n  [Epoch 1/1] Batch 61/844 - Loss: 42.0434\n  [Epoch 1/1] Batch 71/844 - Loss: 38.1708\n  [Epoch 1/1] Batch 81/844 - Loss: 36.7237\n  [Epoch 1/1] Batch 91/844 - Loss: 43.0095\n  [Epoch 1/1] Batch 101/844 - Loss: 30.6881\n  [Epoch 1/1] Batch 111/844 - Loss: 29.9962\n  [Epoch 1/1] Batch 121/844 - Loss: 27.0128\n  [Epoch 1/1] Batch 131/844 - Loss: 28.3021\n  [Epoch 1/1] Batch 141/844 - Loss: 28.6425\n  [Epoch 1/1] Batch 151/844 - Loss: 26.3121\n  [Epoch 1/1] Batch 161/844 - Loss: 30.3331\n  [Epoch 1/1] Batch 171/844 - Loss: 23.8739\n  [Epoch 1/1] Batch 181/844 - Loss: 26.0690\n  [Epoch 1/1] Batch 191/844 - Loss: 23.8308\n  [Epoch 1/1] Batch 201/844 - Loss: 25.1307\n  [Epoch 1/1] Batch 211/844 - Loss: 27.4065\n  [Epoch 1/1] Batch 221/844 - Loss: 25.1697\n  [Epoch 1/1] Batch 231/844 - Loss: 24.4767\n  [Epoch 1/1] Batch 241/844 - Loss: 22.4501\n  [Epoch 1/1] Batch 251/844 - Loss: 20.8791\n  [Epoch 1/1] Batch 261/844 - Loss: 21.1613\n  [Epoch 1/1] Batch 271/844 - Loss: 22.2429\n  [Epoch 1/1] Batch 281/844 - Loss: 20.8988\n  [Epoch 1/1] Batch 291/844 - Loss: 19.5825\n  [Epoch 1/1] Batch 301/844 - Loss: 18.3566\n  [Epoch 1/1] Batch 311/844 - Loss: 18.8548\n  [Epoch 1/1] Batch 321/844 - Loss: 18.8809\n  [Epoch 1/1] Batch 331/844 - Loss: 21.9857\n  [Epoch 1/1] Batch 341/844 - Loss: 17.2394\n  [Epoch 1/1] Batch 351/844 - Loss: 17.0333\n  [Epoch 1/1] Batch 361/844 - Loss: 17.9398\n  [Epoch 1/1] Batch 371/844 - Loss: 19.5333\n  [Epoch 1/1] Batch 381/844 - Loss: 18.4114\n  [Epoch 1/1] Batch 391/844 - Loss: 17.8436\n  [Epoch 1/1] Batch 401/844 - Loss: 17.1047\n  [Epoch 1/1] Batch 411/844 - Loss: 15.4006\n  [Epoch 1/1] Batch 421/844 - Loss: 18.7481\n  [Epoch 1/1] Batch 431/844 - Loss: 17.6505\n  [Epoch 1/1] Batch 441/844 - Loss: 14.9779\n  [Epoch 1/1] Batch 451/844 - Loss: 16.2233\n  [Epoch 1/1] Batch 461/844 - Loss: 16.1939\n  [Epoch 1/1] Batch 471/844 - Loss: 15.8355\n  [Epoch 1/1] Batch 481/844 - Loss: 12.9961\n  [Epoch 1/1] Batch 491/844 - Loss: 15.2155\n  [Epoch 1/1] Batch 501/844 - Loss: 18.2522\n  [Epoch 1/1] Batch 511/844 - Loss: 15.6161\n  [Epoch 1/1] Batch 521/844 - Loss: 15.5039\n  [Epoch 1/1] Batch 531/844 - Loss: 14.5368\n  [Epoch 1/1] Batch 541/844 - Loss: 15.6774\n  [Epoch 1/1] Batch 551/844 - Loss: 16.1399\n  [Epoch 1/1] Batch 561/844 - Loss: 13.6032\n  [Epoch 1/1] Batch 571/844 - Loss: 13.4817\n  [Epoch 1/1] Batch 581/844 - Loss: 16.5305\n  [Epoch 1/1] Batch 591/844 - Loss: 14.4701\n  [Epoch 1/1] Batch 601/844 - Loss: 14.7551\n  [Epoch 1/1] Batch 611/844 - Loss: 14.1337\n  [Epoch 1/1] Batch 621/844 - Loss: 13.9598\n  [Epoch 1/1] Batch 631/844 - Loss: 14.4133\n  [Epoch 1/1] Batch 641/844 - Loss: 13.0329\n  [Epoch 1/1] Batch 651/844 - Loss: 13.6686\n  [Epoch 1/1] Batch 661/844 - Loss: 12.7411\n  [Epoch 1/1] Batch 671/844 - Loss: 13.2199\n  [Epoch 1/1] Batch 681/844 - Loss: 16.2034\n  [Epoch 1/1] Batch 691/844 - Loss: 14.3827\n  [Epoch 1/1] Batch 701/844 - Loss: 13.8049\n  [Epoch 1/1] Batch 711/844 - Loss: 12.0545\n  [Epoch 1/1] Batch 721/844 - Loss: 13.0081\n  [Epoch 1/1] Batch 731/844 - Loss: 14.2586\n  [Epoch 1/1] Batch 741/844 - Loss: 16.0386\n  [Epoch 1/1] Batch 751/844 - Loss: 12.7858\n  [Epoch 1/1] Batch 761/844 - Loss: 13.0571\n  [Epoch 1/1] Batch 771/844 - Loss: 13.1892\n  [Epoch 1/1] Batch 781/844 - Loss: 12.4588\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_path = \"/kaggle/input/cave-hsi\"  # or your real path\nfor root, dirs, files in os.walk(dataset_path):\n    print(f\"üìÇ {root}\")\n    for d in dirs:\n        print(f\"   üìÅ {d}\")\n    for f in files:\n        print(f\"   üìÑ {f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_path = '/kaggle/input/cave-hsi/'\n\n# Check all scene folder paths\nprint(\"üìÇ Checking dataset directory structure:\\n\")\nfor scene_name in os.listdir(dataset_path):\n    outer_path = os.path.join(dataset_path, scene_name)\n    inner_path = os.path.join(outer_path, scene_name)\n    \n    if os.path.isdir(inner_path):\n        print(f\"‚úÖ Found scene folder: {inner_path}\")\n    else:\n        print(f\"‚ùå Skipped: {inner_path} is not a folder\")\n\nprint(\"\\n‚úÖ DONE\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}