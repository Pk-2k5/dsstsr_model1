{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12653491,"sourceType":"datasetVersion","datasetId":7996578}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gputil h5py timm PyWavelets -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:12:27.474069Z","iopub.execute_input":"2025-08-05T08:12:27.474348Z","iopub.status.idle":"2025-08-05T08:13:52.586728Z","shell.execute_reply.started":"2025-08-05T08:12:27.474317Z","shell.execute_reply":"2025-08-05T08:13:52.586017Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q timm --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:13:52.587678Z","iopub.execute_input":"2025-08-05T08:13:52.587899Z","iopub.status.idle":"2025-08-05T08:13:57.494015Z","shell.execute_reply.started":"2025-08-05T08:13:52.587876Z","shell.execute_reply":"2025-08-05T08:13:57.493083Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom scipy.io import loadmat\nimport numpy as np\nimport cv2\nimport os\nimport timm\nfrom timm.models.swin_transformer import SwinTransformerBlock\nimport pywt\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport random\nimport h5py\nimport GPUtil\nfrom torch.amp import GradScaler\nfrom torch import amp\ntorch.set_float32_matmul_precision('medium')   # or 'high' if your GPU supports it\ntorch.backends.cudnn.benchmark = True\n# Suppress the UserWarning from PyWavelets for small patches\nwarnings.filterwarnings(\"ignore\", message=\"Level value of .* is too high: all coefficients will experience boundary effects.\")\n\nprint(\"âœ… Setup complete. All packages installed and libraries imported.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:13:57.496210Z","iopub.execute_input":"2025-08-05T08:13:57.496753Z","iopub.status.idle":"2025-08-05T08:14:11.303595Z","shell.execute_reply.started":"2025-08-05T08:13:57.496727Z","shell.execute_reply":"2025-08-05T08:14:11.302870Z"}},"outputs":[{"name":"stdout","text":"âœ… Setup complete. All packages installed and libraries imported.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def create_lr_hr_pairs(hsi_cube, scale_factor=4):\n    \"\"\"Creates Low-Res (downsampled) and High-Res pairs for training.\"\"\"\n    hr_image = hsi_cube\n    blurred_hr = cv2.GaussianBlur(hr_image, (5, 5), 0)\n    lr_h, lr_w = hr_image.shape[0] // scale_factor, hr_image.shape[1] // scale_factor\n    lr_downsampled = cv2.resize(blurred_hr, (lr_w, lr_h), interpolation=cv2.INTER_CUBIC)\n    return lr_downsampled, hr_image\n\nclass WaveletDenoise3D(nn.Module):\n    \"\"\"Implements the 3D wavelet denoising method from the paper.\"\"\"\n    def __init__(self, wavelet='db4', level=1):\n        super().__init__()\n        self.wavelet = wavelet\n        self.level = level\n        self.coeffs_to_remove = ['aad', 'ada', 'daa', 'ddd']\n\n    def forward(self, x):\n        input_numpy = x.detach().cpu().numpy()\n        output_numpy = np.empty_like(input_numpy)\n        for i in range(input_numpy.shape[0]):\n            volume = input_numpy[i]\n            coeffs = pywt.wavedecn(volume, self.wavelet, level=self.level)\n            for detail_dict in coeffs[1:]:\n                for key in self.coeffs_to_remove:\n                    if key in detail_dict:\n                        detail_dict[key] = np.zeros_like(detail_dict[key])\n            denoised_volume = pywt.waverecn(coeffs, self.wavelet)\n            c, h, w = volume.shape\n            output_numpy[i] = denoised_volume[:c, :h, :w]\n        return torch.from_numpy(output_numpy).to(x.device)\n\ndef denoise_cube(lr_cube):\n    \"\"\"Applies the 3D wavelet denoising to an entire HSI cube one time before training.\"\"\"\n    print(\"Starting one-time denoising of the LR cube...\")\n    temp_tensor = torch.from_numpy(lr_cube).permute(2, 0, 1).unsqueeze(0)\n    denoiser = WaveletDenoise3D()\n    denoised_tensor = denoiser(temp_tensor)\n    denoised_cube = denoised_tensor.squeeze(0).permute(1, 2, 0).numpy()\n    print(\"Denoising complete.\")\n    return denoised_cube\n\nprint(\"âœ… Data preparation functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:11.304241Z","iopub.execute_input":"2025-08-05T08:14:11.304630Z","iopub.status.idle":"2025-08-05T08:14:11.312927Z","shell.execute_reply.started":"2025-08-05T08:14:11.304611Z","shell.execute_reply":"2025-08-05T08:14:11.312184Z"}},"outputs":[{"name":"stdout","text":"âœ… Data preparation functions defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport cv2\n\ndef load_scene_from_pngs(scene_path):\n    \"\"\"Load a 31-band HSI scene from sorted PNGs in a directory.\"\"\"\n    # âš ï¸ Only include files that match the *_ms_XX.png format\n    png_files = [\n        f for f in os.listdir(scene_path)\n        if f.endswith('.png') and '_ms_' in f\n    ]\n    \n    # âœ… Sort by band number extracted from filename\n    def band_number(filename):\n        parts = filename.split('_')\n        band_str = parts[-1].replace('.png', '')\n        return int(band_str)\n\n    png_files = sorted(png_files, key=band_number)\n\n    if len(png_files) != 31:\n        print(f\"    âŒ Skipping: Found {len(png_files)} bands, expected 31.\")\n        return None\n\n    first_img_path = os.path.join(scene_path, png_files[0])\n    sample_img = cv2.imread(first_img_path, cv2.IMREAD_GRAYSCALE)\n    if sample_img is None:\n        print(f\"    âŒ Corrupt image: {png_files[0]}\")\n        return None\n    \n    h, w = sample_img.shape\n    hsi_cube = np.zeros((h, w, 31), dtype=np.float32)\n\n    for i, fname in enumerate(png_files):\n        file_path = os.path.join(scene_path, fname)\n        band_img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n        if band_img is None:\n            print(f\"    âš ï¸ Skipping corrupt image: {file_path}\")\n            return None\n        # âœ… Normalize 8-bit image\n        hsi_cube[:, :, i] = band_img.astype(np.float32) / 255.0\n\n    return hsi_cube\n\n# Dummy placeholders â€“ replace with your actual functions\ndef create_lr_hr_pairs(hsi_cube, scale_factor):\n    hr = hsi_cube[np.newaxis, ...]  # Add batch dimension\n    lr = cv2.resize(hsi_cube, (hsi_cube.shape[1] // scale_factor, hsi_cube.shape[0] // scale_factor))\n    lr = cv2.resize(lr, (hsi_cube.shape[1], hsi_cube.shape[0]))  # Upsample back\n    lr = lr[np.newaxis, ...]\n    return lr, hr\n\ndef denoise_cube(cube):\n    return cube  # Placeholder (no denoising)\n\ndef prepare_data_cubes_from_folders(scene_folder_list, scale_factor=4):\n    hr_cubes, lr_denoised_cubes = [], []\n\n    for scene_path in scene_folder_list:\n        print(f\"  Processing: {os.path.basename(scene_path)}\")\n        hsi_cube = load_scene_from_pngs(scene_path)\n\n        if hsi_cube is None:\n            continue\n\n        # Crop to multiple of scale\n        h, w, c = hsi_cube.shape\n        hsi_cube = hsi_cube[:h - (h % scale_factor), :w - (w % scale_factor), :]\n\n        # Normalize each band (again, optional â€” already normalized)\n        for i in range(c):\n            band = hsi_cube[:, :, i]\n            min_val, max_val = np.min(band), np.max(band)\n            if max_val > min_val:\n                hsi_cube[:, :, i] = (band - min_val) / (max_val - min_val)\n\n        lr_down, hr = create_lr_hr_pairs(hsi_cube, scale_factor=scale_factor)\n        lr_denoised = denoise_cube(lr_down)\n\n        hr_cubes.append(hr)\n        lr_denoised_cubes.append(lr_denoised)\n\n    if not hr_cubes:\n        raise ValueError(\"âŒ No valid 31-band scenes found.\")\n\n    return np.concatenate(hr_cubes, axis=0), np.concatenate(lr_denoised_cubes, axis=0)\n\n# --- Traverse the double-nested dataset structure ---\ndataset_path = '/kaggle/input/cave-hsi/'\n\nscene_folders = []\n\nfor scene_name in os.listdir(dataset_path):\n    outer_path = os.path.join(dataset_path, scene_name)\n    inner_path = os.path.join(outer_path, scene_name)\n    \n    if os.path.isdir(inner_path):\n        png_files = [f for f in os.listdir(inner_path) if f.endswith('.png') and '_ms_' in f]\n        if len(png_files) == 31:\n            scene_folders.append(inner_path)\n        else:\n            print(f\"âš ï¸ Skipping {scene_name}: found {len(png_files)} PNGs, expected 31.\")\n\nprint(f\"âœ… Ready to process {len(scene_folders)} valid scenes.\")\nhr_cubes, lr_denoised_cubes = prepare_data_cubes_from_folders(scene_folders)\nprint(f\"âœ… HR shape: {hr_cubes.shape}, LR shape: {lr_denoised_cubes.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:11.313751Z","iopub.execute_input":"2025-08-05T08:14:11.313954Z","iopub.status.idle":"2025-08-05T08:14:36.098053Z","shell.execute_reply.started":"2025-08-05T08:14:11.313939Z","shell.execute_reply":"2025-08-05T08:14:36.097243Z"}},"outputs":[{"name":"stdout","text":"âœ… Ready to process 32 valid scenes.\n  Processing: oil_painting_ms\n  Processing: superballs_ms\n  Processing: egyptian_statue_ms\n  Processing: fake_and_real_tomatoes_ms\n  Processing: photo_and_face_ms\n  Processing: glass_tiles_ms\n  Processing: beads_ms\n  Processing: fake_and_real_lemon_slices_ms\n  Processing: hairs_ms\n  Processing: chart_and_stuffed_toy_ms\n  Processing: watercolors_ms\n  Processing: clay_ms\n  Processing: stuffed_toys_ms\n  Processing: fake_and_real_peppers_ms\n  Processing: fake_and_real_strawberries_ms\n  Processing: sponges_ms\n  Processing: face_ms\n  Processing: cd_ms\n  Processing: fake_and_real_beers_ms\n  Processing: real_and_fake_apples_ms\n  Processing: feathers_ms\n  Processing: fake_and_real_food_ms\n  Processing: jelly_beans_ms\n  Processing: balloons_ms\n  Processing: thread_spools_ms\n  Processing: flowers_ms\n  Processing: paints_ms\n  Processing: pompoms_ms\n  Processing: fake_and_real_sushi_ms\n  Processing: cloth_ms\n  Processing: real_and_fake_peppers_ms\n  Processing: fake_and_real_lemons_ms\nâœ… HR shape: (32, 512, 512, 31), LR shape: (32, 512, 512, 31)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class HSISuperResolutionDataset(Dataset):\n    def __init__(self, hr_cube, lr_downsampled_cube, hr_patch_size=63, scale_factor=4):\n        self.hr_cube = hr_cube\n        self.lr_cube = lr_downsampled_cube\n        self.hr_patch_size = hr_patch_size\n        self.lr_patch_size = hr_patch_size // scale_factor\n        self.scale_factor = scale_factor\n\n        self.n_scenes, self.h, self.w, self.bands = self.hr_cube.shape\n        self.patch_step = self.hr_patch_size // 2\n        self.band_groups = self.bands // 5\n        self.patch_coords = [\n            (scene_idx, r, c)\n            for scene_idx in range(self.n_scenes)\n            for r in range(0, self.h - self.hr_patch_size + 1, self.patch_step)\n            for c in range(0, self.w - self.hr_patch_size + 1, self.patch_step)\n        ]\n        self.total_samples = len(self.patch_coords) * self.band_groups\n\n    def __len__(self):\n        return self.total_samples\n\n    def __getitem__(self, index):\n        patch_index = index % len(self.patch_coords)\n        band_group_index = index // len(self.patch_coords)\n        scene_idx, r, c = self.patch_coords[patch_index]\n        hr_patch = self.hr_cube[scene_idx, r:r + self.hr_patch_size, c:c + self.hr_patch_size, :]\n        lr_patch = self.lr_cube[scene_idx,\n                                r // self.scale_factor:r // self.scale_factor + self.lr_patch_size,\n                                c // self.scale_factor:c // self.scale_factor + self.lr_patch_size, :]\n        start_band = band_group_index * 5\n        end_band = start_band + 5\n        hr_patch = hr_patch[:, :, start_band:end_band]\n        lr_patch = lr_patch[:, :, start_band:end_band]\n\n        if np.random.rand() > 0.5:\n            lr_patch = np.ascontiguousarray(np.flip(lr_patch, axis=1))\n            hr_patch = np.ascontiguousarray(np.flip(hr_patch, axis=1))\n        if np.random.rand() > 0.5:\n            lr_patch = np.ascontiguousarray(np.flip(lr_patch, axis=0))\n            hr_patch = np.ascontiguousarray(np.flip(hr_patch, axis=0))\n\n        lr_tensor = torch.from_numpy(lr_patch.copy()).float().permute(2, 0, 1)\n        center_band_idx = hr_patch.shape[2] // 2\n        hr_tensor = torch.from_numpy(hr_patch[:, :, center_band_idx:center_band_idx+1].copy()).float().permute(2, 0, 1)\n        return lr_tensor, hr_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.099038Z","iopub.execute_input":"2025-08-05T08:14:36.099330Z","iopub.status.idle":"2025-08-05T08:14:36.108364Z","shell.execute_reply.started":"2025-08-05T08:14:36.099304Z","shell.execute_reply":"2025-08-05T08:14:36.107584Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ShallowFeatureExtractor(nn.Module):\n    def __init__(self, in_channels=5, base_channels=64):\n        super().__init__()\n        self.conv2d_initial = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        self.res_block = nn.Sequential(\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1)\n        )\n        self.spatial_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(3, 3, 3), padding=1)\n        self.spectral_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(3, 3, 3), padding=1)\n        self.fusion_conv3d = nn.Conv3d(base_channels, base_channels, kernel_size=(1, 1, 1))\n\n    def forward(self, x):\n        t = self.conv2d_initial(x)\n        t = t + self.res_block(t)\n        t_3d = t.unsqueeze(2)\n        feat_spa = self.spatial_conv3d(t_3d)\n        feat_spec = self.spectral_conv3d(t_3d)\n        fused_feat = feat_spa + feat_spec\n        return self.fusion_conv3d(fused_feat).squeeze(2)\n\nclass SwinTransformerBlockWrapper(nn.Module):\n    def __init__(self, dim, num_heads, window_size=7):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n\n    def forward(self, x):\n        B, N, C = x.shape\n        H = W = int(np.sqrt(N))\n        swin_block = SwinTransformerBlock(\n            dim=self.dim,\n            input_resolution=(H, W),\n            num_heads=self.num_heads,\n            window_size=7,\n            shift_size=3\n        ).to(x.device)\n        x = x.view(B, H, W, C)\n        x = swin_block(x)\n        return x.view(B, -1, C)\n\nclass SpectralAttention(nn.Module):\n    def __init__(self, dim, num_heads=8):\n        super().__init__()\n        assert dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.proj = nn.Linear(dim, dim)\n        self.sigma = nn.Parameter(torch.ones(1))  # Learnable\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (self.sigma * q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        return self.proj(x)\nclass DSAL(nn.Module):\n    def __init__(self, dim, num_heads, window_size=7):\n        super().__init__()\n        self.norm1, self.norm2 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n        self.norm3, self.norm4 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n        self.spatial_attention = SwinTransformerBlockWrapper(dim, num_heads, window_size=7)\n        self.spectral_attention = SpectralAttention(dim, num_heads)\n        self.mlp1 = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n        self.mlp2 = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n\n    def forward(self, x):\n        x = x + self.spatial_attention(self.norm1(x))\n        x = x + self.mlp1(self.norm2(x))\n        x = x + self.spectral_attention(self.norm3(x))\n        x = x + self.mlp2(self.norm4(x))\n        return x\n\nclass DSST_Module(nn.Module):\n    def __init__(self, dim, num_heads, num_dsal=6, window_size=7):\n        super().__init__()\n        self.dsal_blocks = nn.ModuleList([DSAL(dim, num_heads, window_size=7) for _ in range(num_dsal)])\n        self.conv_final = nn.Conv2d(dim, dim, 3, padding=1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x_res = x\n        x = x.flatten(2).transpose(1, 2)\n        for block in self.dsal_blocks:\n            x = block(x)\n        x = x.transpose(1, 2).reshape(B, C, H, W)\n        return self.conv_final(x) + x_res\n        \nclass DSSTSR(nn.Module):\n    def __init__(self, in_channels=5, base_channels=64, out_channels=1, scale_factor=4, num_dsst_modules=4, window_size=7):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.shallow_extractor = ShallowFeatureExtractor(in_channels=in_channels, base_channels=base_channels)\n        self.deep_extractor = nn.Sequential(*[DSST_Module(base_channels, num_heads=8, window_size=window_size)\n                                              for _ in range(num_dsst_modules)])\n        self.upsampler = nn.Sequential(\n            nn.Conv2d(base_channels, out_channels * (scale_factor ** 2), 3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n\n    def forward(self, x):\n        center_band_idx = x.shape[1] // 2\n        bicubic_upsampled_input = F.interpolate(\n            x[:, center_band_idx:center_band_idx+1, :, :], \n            scale_factor=self.scale_factor, \n            mode='bicubic', \n            align_corners=False\n        )\n        shallow_features = self.shallow_extractor(x)\n        deep_features = self.deep_extractor(shallow_features)\n        residual = self.upsampler(deep_features)\n        return bicubic_upsampled_input + residual\n\nprint(\"âœ… Model architecture classes defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.109121Z","iopub.execute_input":"2025-08-05T08:14:36.109390Z","iopub.status.idle":"2025-08-05T08:14:36.143293Z","shell.execute_reply.started":"2025-08-05T08:14:36.109368Z","shell.execute_reply":"2025-08-05T08:14:36.142731Z"}},"outputs":[{"name":"stdout","text":"âœ… Model architecture classes defined.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class HSI_Classifier(nn.Module):\n    def __init__(self, num_bands, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(num_bands, 100)\n        self.fc2 = nn.Linear(100, num_classes)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\nclass PixelDataset(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = torch.from_numpy(X).float(), torch.from_numpy(y).long()\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ndef run_classification_evaluation(sr_image_cube, gt_path):\n    print(\"\\n--- Starting Classification Evaluation ---\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load ground truth\n    gt_data = loadmat(gt_path)\n    gt_map = gt_data['indian_pines_gt']\n    \n    # --- ADD THIS CODE TO CROP THE GROUND TRUTH MAP ---\n    # Ensure gt_map has the same spatial dimensions as the sr_image_cube\n    h, w, _ = sr_image_cube.shape\n    gt_map = gt_map[:h, :w]\n    # --- End of New Code ---\n    \n    # Prepare pixel data (this line will now work correctly)\n    X_pixels, y_pixels = sr_image_cube[gt_map > 0], gt_map[gt_map > 0] - 1 \n    num_classes = len(np.unique(y_pixels))\n    \n    # Paper's \"30 samples per class\" strategy\n    train_size = 30 * num_classes\n    if train_size >= len(y_pixels): # Make sure train_size is not too large\n        train_size = int(len(y_pixels) * 0.5)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_pixels, y_pixels, train_size=train_size, stratify=y_pixels, random_state=42\n    )\n    \n    train_loader = DataLoader(PixelDataset(X_train, y_train), batch_size=128, shuffle=True)\n    test_loader = DataLoader(PixelDataset(X_test, y_test), batch_size=128, shuffle=False)\n    \n    classifier = HSI_Classifier(num_bands=sr_image_cube.shape[2], num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n    \n    print(\"Training classifier...\")\n    for epoch in range(100): # Train for a reasonable number of epochs\n        for data, labels in train_loader:\n            data, labels = data.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = classifier(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n    \n    print(\"Evaluating classifier...\")\n    classifier.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for data, labels in test_loader:\n            data = data.to(device)\n            outputs = classifier(data)\n            _, predicted = torch.max(outputs.data, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    oa = accuracy_score(all_labels, all_preds)\n    kappa = cohen_kappa_score(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n    aa = np.mean([report[str(i)]['recall'] for i in range(num_classes)])\n    \n    return {'OA': oa, 'AA': aa, 'Kappa': kappa}\nprint(\"âœ… Classification experiment functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.143957Z","iopub.execute_input":"2025-08-05T08:14:36.144160Z","iopub.status.idle":"2025-08-05T08:14:36.169915Z","shell.execute_reply.started":"2025-08-05T08:14:36.144133Z","shell.execute_reply":"2025-08-05T08:14:36.169322Z"}},"outputs":[{"name":"stdout","text":"âœ… Classification experiment functions defined.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport time\nimport gc\n\ndef t4_optimized_reconstruction(model, lr_scene, hr_shape, scale_factor, device):\n    h, w, bands = hr_shape\n    reconstructed = np.zeros((h, w, bands), dtype=np.float32)\n    weight_map = np.zeros_like(reconstructed)\n\n    print(f\" T4-optimized reconstruction: {h}Ã—{w}Ã—{bands}\")\n    start_time = time.time()\n\n    patch_size = 63  # as per paper\n    step_size = 48   # as per paper\n\n    lr_h, lr_w = lr_scene.shape[:2]\n\n    torch.cuda.empty_cache()\n    total_memory = torch.cuda.get_device_properties(device).total_memory\n    allocated_memory = torch.cuda.memory_allocated(device)\n    free_memory = total_memory - allocated_memory\n\n    if free_memory > 8e9:\n        initial_batch_size = 12\n    elif free_memory > 4e9:\n        initial_batch_size = 8\n    else:\n        initial_batch_size = 4\n\n    print(f\" Available memory: {free_memory/1e9:.1f}GB, using batch size: {initial_batch_size}\")\n\n    # Cover entire image including edges\n    patch_coords = []\n    for r in range(0, lr_h - patch_size + 1, step_size):\n        for c in range(0, lr_w - patch_size + 1, step_size):\n            patch_coords.append((r, c))\n    if (lr_h - patch_size) % step_size != 0:\n        for c in range(0, lr_w - patch_size + 1, step_size):\n            patch_coords.append((lr_h - patch_size, c))\n    if (lr_w - patch_size) % step_size != 0:\n        for r in range(0, lr_h - patch_size + 1, step_size):\n            patch_coords.append((r, lr_w - patch_size))\n    if (lr_h - patch_size) % step_size != 0 and (lr_w - patch_size) % step_size != 0:\n        patch_coords.append((lr_h - patch_size, lr_w - patch_size))\n\n    print(f\" Processing {len(patch_coords)} patches across {bands} bands\")\n\n    model.eval()\n    with torch.no_grad():\n        band_to_input_bands = {}\n        for band_idx in range(bands):\n            if band_idx < 2:\n                input_bands = [0, 1, 2, 3, 4]\n            elif band_idx > bands - 3:\n                input_bands = [bands-5, bands-4, bands-3, bands-2, bands-1]\n            else:\n                input_bands = [band_idx-2, band_idx-1, band_idx, band_idx+1, band_idx+2]\n            band_to_input_bands[band_idx] = tuple(input_bands)\n\n        input_to_output_bands = {}\n        for output_band, input_bands in band_to_input_bands.items():\n            input_to_output_bands.setdefault(input_bands, []).append(output_band)\n\n        print(f\" Optimization: {bands} bands grouped into {len(input_to_output_bands)} unique inputs\")\n\n        for input_idx, (input_bands, output_bands) in enumerate(input_to_output_bands.items()):\n            print(f\" Processing input {input_idx+1}/{len(input_to_output_bands)}: bands {input_bands} â†’ output bands {output_bands}\")\n            lr_input_5bands = lr_scene[:, :, list(input_bands)]\n\n            all_patches = []\n            valid_coords = []\n\n            for r, c in patch_coords:\n                patch = lr_input_5bands[r:r+patch_size, c:c+patch_size, :]\n                if patch.shape[:2] == (patch_size, patch_size):\n                    all_patches.append(patch)\n                    valid_coords.append((r, c))\n\n            if not all_patches:\n                continue\n\n            current_batch_size = initial_batch_size\n\n            for batch_start in range(0, len(all_patches), current_batch_size):\n                batch_end = min(batch_start + current_batch_size, len(all_patches))\n                batch_patches = all_patches[batch_start:batch_end]\n                batch_coords = valid_coords[batch_start:batch_end]\n\n                success = False\n                retry_count = 0\n\n                while not success and retry_count < 3:\n                    try:\n                        batch_tensor = torch.stack([\n                            torch.from_numpy(patch).float().permute(2, 0, 1)\n                            for patch in batch_patches\n                        ]).to(device, non_blocking=True)\n\n                        with torch.amp.autocast('cuda'):\n                            hr_batch = model(batch_tensor)\n\n                        hr_batch_cpu = hr_batch.cpu().numpy()\n\n                        for i, (r, c) in enumerate(batch_coords):\n                            hr_patch = hr_batch_cpu[i, 0]\n\n                            hr_r = r * scale_factor\n                            hr_c = c * scale_factor\n                            hr_r_end = min(hr_r + hr_patch.shape[0], h)\n                            hr_c_end = min(hr_c + hr_patch.shape[1], w)\n\n                            patch_h = hr_r_end - hr_r\n                            patch_w = hr_c_end - hr_c\n\n                            if patch_h > 0 and patch_w > 0:\n                                for output_band in output_bands:\n                                    reconstructed[hr_r:hr_r_end, hr_c:hr_c_end, output_band] += hr_patch[:patch_h, :patch_w]\n                                    weight_map[hr_r:hr_r_end, hr_c:hr_c_end, output_band] += 1\n\n                        success = True\n\n                    except RuntimeError as e:\n                        if \"out of memory\" in str(e):\n                            retry_count += 1\n                            current_batch_size = max(1, current_batch_size // 2)\n                            print(f\" OOM detected, reducing batch size to {current_batch_size}, retry {retry_count}/3\")\n                            torch.cuda.empty_cache()\n                            gc.collect()\n                        else:\n                            raise e\n\n                if batch_start % (current_batch_size * 4) == 0:\n                    torch.cuda.empty_cache()\n\n            print(f\" Completed input {input_idx+1}/{len(input_to_output_bands)}\")\n\n    # Normalize overlapping contributions\n    weight_map[weight_map == 0] = 1  # avoid division by zero\n    reconstructed /= weight_map\n\n    total_time = time.time() - start_time\n    print(f\" T4-optimized reconstruction completed in {total_time:.1f} seconds\")\n\n    return reconstructed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.172041Z","iopub.execute_input":"2025-08-05T08:14:36.172228Z","iopub.status.idle":"2025-08-05T08:14:36.194761Z","shell.execute_reply.started":"2025-08-05T08:14:36.172214Z","shell.execute_reply":"2025-08-05T08:14:36.194198Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Debug: Check what model files exist\nimport os\nprint(\"\\nðŸ” Checking for saved model files...\")\npotential_files = [\n    'best_dsstsr_weights.pth',\n    'best_dsstsr_model_fixed.pth', \n    'final_dsstsr_checkpoint.pth',\n    'early_stopped_model.pth'\n]\n\navailable_files = []\nfor file in potential_files:\n    if os.path.exists(file):\n        file_size = os.path.getsize(file) / (1024*1024)  # MB\n        print(f\"  âœ… Found: {file} ({file_size:.1f} MB)\")\n        available_files.append(file)\n    else:\n        print(f\"  âŒ Missing: {file}\")\n\nif available_files:\n    model_path = available_files[0]  # Use the first available file\n    print(f\"\\nðŸ“ Using model file: {model_path}\")\nelse:\n    print(\"\\nâš ï¸ No model files found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.195434Z","iopub.execute_input":"2025-08-05T08:14:36.195630Z","iopub.status.idle":"2025-08-05T08:14:36.221514Z","shell.execute_reply.started":"2025-08-05T08:14:36.195608Z","shell.execute_reply":"2025-08-05T08:14:36.220976Z"}},"outputs":[{"name":"stdout","text":"\nðŸ” Checking for saved model files...\n  âŒ Missing: best_dsstsr_weights.pth\n  âŒ Missing: best_dsstsr_model_fixed.pth\n  âŒ Missing: final_dsstsr_checkpoint.pth\n  âŒ Missing: early_stopped_model.pth\n\nâš ï¸ No model files found!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import concurrent.futures\nimport threading\n\ndef process_scene_on_gpu(gpu_id, model_path, lr_scene, hr_shape, scale_factor, scene_name):\n    \"\"\"Process one scene on specific GPU with robust model loading\"\"\"\n    device = torch.device(f'cuda:{gpu_id}')\n    torch.cuda.set_device(gpu_id)\n    torch.cuda.empty_cache()\n    \n    # Load model on this GPU\n    model = DSSTSR(in_channels=5, out_channels=1, base_channels=64, scale_factor=scale_factor)\n    \n    try:\n        # Load the model file\n        checkpoint = torch.load(model_path, map_location=device)\n        \n        # Handle both checkpoint and weights-only files\n        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n            # This is a full checkpoint\n            state_dict = checkpoint['model_state_dict']\n            epoch_info = checkpoint.get('epoch', 'unknown')\n            val_loss = checkpoint.get('val_loss', 'unknown')\n            print(f\"    GPU {gpu_id}: Loaded checkpoint from epoch {epoch_info} (val_loss: {val_loss})\")\n        else:\n            # This is weights-only\n            state_dict = checkpoint\n            print(f\"    GPU {gpu_id}: Loaded weights-only file\")\n        \n        # Handle DataParallel state dict (remove 'module.' prefix)\n        if any(k.startswith('module.') for k in state_dict.keys()):\n            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n            print(f\"    GPU {gpu_id}: Removed DataParallel prefix from state dict\")\n        \n        # Load the state dict into model\n        model.load_state_dict(state_dict)\n        print(f\"    GPU {gpu_id}: âœ… Model loaded successfully for {scene_name}\")\n        \n    except Exception as e:\n        print(f\"    GPU {gpu_id}: âŒ Error loading model: {e}\")\n        print(f\"    GPU {gpu_id}: âš ï¸ Using randomly initialized model for {scene_name}\")\n    \n    model.to(device)\n    model.eval()\n    \n    print(f\"    GPU {gpu_id}: ðŸ”„ Processing {scene_name}...\")\n    \n    # Run reconstruction\n    try:\n        result = t4_optimized_reconstruction(model, lr_scene, hr_shape, scale_factor, device)\n        print(f\"    GPU {gpu_id}: âœ… Completed {scene_name}\")\n        return scene_name, result\n    \n    except Exception as e:\n        print(f\"    GPU {gpu_id}: âŒ Error during reconstruction of {scene_name}: {e}\")\n        # Return dummy result to prevent the entire process from failing\n        dummy_result = torch.zeros(hr_shape, dtype=torch.float32)\n        return scene_name, dummy_result.numpy()\n\ndef parallel_scene_processing(scenes_data, model_path, scale_factor=4):\n    \"\"\"Process scenes in parallel across GPUs with better error handling\"\"\"\n    num_gpus = torch.cuda.device_count()\n    print(f\"ðŸš€ Starting parallel processing on {num_gpus} GPUs\")\n    print(f\"ðŸ“ Using model file: {model_path}\")\n    \n    # Verify model file exists\n    if not os.path.exists(model_path):\n        print(f\"âŒ Model file not found: {model_path}\")\n        available_models = [f for f in os.listdir('.') if f.endswith('.pth')]\n        if available_models:\n            print(f\"Available .pth files: {available_models}\")\n            model_path = available_models[0]\n            print(f\"ðŸ”„ Using fallback model: {model_path}\")\n        else:\n            raise FileNotFoundError(f\"No model file found: {model_path}\")\n    \n    # Show model file info\n    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n    print(f\"ðŸ“Š Model file size: {model_size_mb:.1f} MB\")\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_gpus) as executor:\n        print(f\"âš¡ Submitting {len(scenes_data)} scenes to {num_gpus} GPUs...\")\n        futures = []\n        \n        for i, (lr_scene, hr_shape, scene_name) in enumerate(scenes_data):\n            gpu_id = i % num_gpus  # Round-robin GPU assignment\n            print(f\"  ðŸ“¤ Submitting {scene_name} to GPU {gpu_id}\")\n            \n            future = executor.submit(\n                process_scene_on_gpu, \n                gpu_id, model_path, lr_scene, hr_shape, scale_factor, scene_name\n            )\n            futures.append(future)\n        \n        # Collect results with progress tracking\n        results = {}\n        completed = 0\n        total = len(futures)\n        \n        print(f\"\\nðŸ”„ Processing {total} scenes...\")\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                scene_name, result = future.result()\n                results[scene_name] = result\n                completed += 1\n                print(f\"  âœ… [{completed}/{total}] Completed: {scene_name}\")\n                \n            except Exception as e:\n                completed += 1\n                print(f\"  âŒ [{completed}/{total}] Failed: {e}\")\n                continue\n    \n    print(f\"\\nðŸŽ‰ Parallel processing complete! Processed {len(results)}/{total} scenes successfully\")\n    return results\n\n# Helper function to find the best model file\ndef find_best_model_file():\n    \"\"\"Find the best available model file in priority order\"\"\"\n    model_candidates = [\n        ('best_dsstsr_weights.pth', 'Best model weights only'),\n        ('best_dsstsr_model_fixed.pth', 'Best model checkpoint'), \n        ('final_dsstsr_checkpoint.pth', 'Final training checkpoint'),\n        ('early_stopped_model.pth', 'Early stopped model')\n    ]\n    \n    print(\"\\nðŸ” Searching for model files...\")\n    for model_path, description in model_candidates:\n        if os.path.exists(model_path):\n            size_mb = os.path.getsize(model_path) / (1024 * 1024)\n            print(f\"  âœ… Found: {model_path} ({size_mb:.1f} MB) - {description}\")\n            return model_path\n        else:\n            print(f\"  âŒ Missing: {model_path}\")\n    \n    # If no standard files found, look for any .pth file\n    pth_files = [f for f in os.listdir('.') if f.endswith('.pth')]\n    if pth_files:\n        fallback = pth_files[0]\n        print(f\"  ðŸ”„ Using fallback: {fallback}\")\n        return fallback\n    \n    raise FileNotFoundError(\"No model files found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.222216Z","iopub.execute_input":"2025-08-05T08:14:36.222478Z","iopub.status.idle":"2025-08-05T08:14:36.245857Z","shell.execute_reply.started":"2025-08-05T08:14:36.222454Z","shell.execute_reply":"2025-08-05T08:14:36.245324Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nfrom torch.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader\n\ndef init_weights(m):\n    \"\"\"Applies Kaiming initialization to conv and linear layers.\"\"\"\n    if isinstance(m, (nn.Conv2d, nn.Conv3d, nn.Linear)):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n            \ndef compute_metrics(y_true, y_pred):\n    y_true, y_pred = np.clip(y_true, 0, 1), np.clip(y_pred, 0, 1)\n\n    psnr_val = psnr(y_true, y_pred, data_range=1.0)\n\n    ssim_val = ssim(\n        y_true, y_pred, data_range=1.0, multichannel=True, channel_axis=-1, win_size=11\n    )\n\n    y_true_flat = y_true.reshape(-1, y_true.shape[-1])\n    y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n\n    dot_products = np.sum(y_true_flat * y_pred_flat, axis=1)\n    norms_true = np.linalg.norm(y_true_flat, axis=1)\n    norms_pred = np.linalg.norm(y_pred_flat, axis=1)\n\n    valid_mask = (norms_true > 1e-10) & (norms_pred > 1e-10)\n    sam_values = np.zeros(len(dot_products))\n\n    if np.any(valid_mask):\n        cos_angles = dot_products[valid_mask] / (norms_true[valid_mask] * norms_pred[valid_mask])\n        cos_angles = np.clip(cos_angles, -1, 1)\n        sam_values[valid_mask] = np.arccos(cos_angles)\n\n    sam_mean = np.mean(sam_values) * 180 / np.pi\n    return {'PSNR': psnr_val, 'SSIM': ssim_val, 'SAM': sam_mean}\n\nclass EarlyStopping:\n    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n    def __init__(self, patience=20, min_delta=0.001, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        \"\"\"Returns True if training should stop, False otherwise\"\"\"\n        if val_loss < self.best_loss - self.min_delta:\n            # Improvement found\n            self.best_loss = val_loss\n            self.counter = 0\n            if self.restore_best_weights:\n                self.best_weights = model.state_dict().copy()\n            return False\n        else:\n            # No improvement\n            self.counter += 1\n            if self.counter >= self.patience:\n                if self.restore_best_weights and self.best_weights is not None:\n                    print(f\"Restoring best model weights from validation loss: {self.best_loss:.6f}\")\n                    model.load_state_dict(self.best_weights)\n                return True\n            return False\n\ndef find_best_model_file():\n    \"\"\"Find the best available model file in priority order\"\"\"\n    model_candidates = [\n        ('best_dsstsr_weights.pth', 'Best model weights only'),\n        ('best_dsstsr_model_fixed.pth', 'Best model checkpoint'), \n        ('final_dsstsr_checkpoint.pth', 'Final training checkpoint'),\n        ('early_stopped_model.pth', 'Early stopped model')\n    ]\n    \n    print(\"\\nðŸ” Searching for model files...\")\n    for model_path, description in model_candidates:\n        if os.path.exists(model_path):\n            size_mb = os.path.getsize(model_path) / (1024 * 1024)\n            print(f\"  âœ… Found: {model_path} ({size_mb:.1f} MB) - {description}\")\n            return model_path\n        else:\n            print(f\"  âŒ Missing: {model_path}\")\n    \n    # If no standard files found, look for any .pth file\n    pth_files = [f for f in os.listdir('.') if f.endswith('.pth')]\n    if pth_files:\n        fallback = pth_files[0]\n        print(f\"  ðŸ”„ Using fallback: {fallback}\")\n        return fallback\n    \n    raise FileNotFoundError(\"No model files found!\")\n\ndef run_experiment_fixed(hr_train_cube, lr_train_denoised_cube, test_files, num_epochs=175, scale_factor=4, hr_patch_size=63, val_folders=None):\n    import time, os, cv2\n    import torch\n    import numpy as np\n    from torch.amp import autocast, GradScaler\n    from torch.utils.data import DataLoader\n    \n    print(\"\\n--- Starting Super-Resolution Experiment (Optimized Version) ---\")\n    lr_patch_size = hr_patch_size // scale_factor\n    \n    # Prepare validation data\n    if val_folders is not None:\n        print(\"Using provided validation folders...\")\n        hr_val_scene, lr_val_denoised_scene = prepare_data_cubes_from_folders(val_folders[:1])  # Use first val folder\n    else:\n        print(\"Preparing validation data from the first test scene...\")\n        hr_val_scene, lr_val_denoised_scene = prepare_data_cubes_from_folders([test_files[0]], scale_factor=scale_factor)\n\n    train_dataset = HSISuperResolutionDataset(hr_train_cube, lr_train_denoised_cube, hr_patch_size, scale_factor)\n    val_dataset = HSISuperResolutionDataset(hr_val_scene, lr_val_denoised_scene, hr_patch_size, scale_factor)\n\n    # Optimized data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=96,              # Increased batch size\n        shuffle=True, \n        num_workers=4,               # More workers\n        pin_memory=True, \n        prefetch_factor=8,           # Higher prefetch\n        persistent_workers=True,     # Keep workers alive\n        drop_last=True              # Consistent batch sizes\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=96, \n        shuffle=False, \n        num_workers=4, \n        pin_memory=True\n    )\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def paper_faithful_init(m):\n        if isinstance(m, (nn.Conv2d, nn.Conv3d)):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    \n    model = DSSTSR(in_channels=5, out_channels=1, base_channels=64, scale_factor=scale_factor, window_size=7)\n    model.apply(paper_faithful_init)\n    model.to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Activating data parallelism on {torch.cuda.device_count()} GPUs!\")\n        model = torch.nn.DataParallel(model)\n\n    criterion = torch.nn.L1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    \n    # Optimized learning rate schedule with warmup\n    def lr_lambda(epoch):\n        if epoch < 10:  # Warmup\n            return (epoch + 1) / 10\n        else:\n            return 0.5 ** ((epoch - 10) // 25)  # Faster decay\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = GradScaler('cuda')\n\n    # Initialize early stopping\n    early_stopping = EarlyStopping(\n        patience=15,           # Reduced patience\n        min_delta=0.001,      # Minimum improvement threshold\n        restore_best_weights=True\n    )\n\n    best_val_loss = float('inf')\n    best_epoch = 0\n    validation_frequency = 5  # Validate every 5 epochs\n    \n    print(\"Starting training with early stopping...\")\n    print(f\"Early stopping: patience={early_stopping.patience}, validation every {validation_frequency} epochs\")\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_loss = 0.0\n        epoch_start_time = time.time()\n        batch_losses = []\n\n        for i, (lr_batch, hr_batch) in enumerate(train_loader):\n            # Convert to channels_last for better performance\n            lr_batch = lr_batch.to(device, memory_format=torch.channels_last)\n            hr_batch = hr_batch.to(device, memory_format=torch.channels_last)\n            \n            optimizer.zero_grad()\n            \n            with autocast(device_type='cuda'):\n                preds = model(lr_batch)\n                min_h = min(preds.shape[2], hr_batch.shape[2])\n                min_w = min(preds.shape[3], hr_batch.shape[3])\n                preds = preds[:, :, :min_h, :min_w]\n                hr_batch = hr_batch[:, :, :min_h, :min_w]\n                loss = criterion(preds, hr_batch)\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            batch_losses.append(loss.item())\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n\n        # Validation phase - only every N epochs to save time\n        if epoch % validation_frequency == 0:\n            model.eval()\n            val_loss = 0.0\n            with torch.no_grad():\n                for lr_batch, hr_batch in val_loader:\n                    lr_batch = lr_batch.to(device, memory_format=torch.channels_last)\n                    hr_batch = hr_batch.to(device, memory_format=torch.channels_last)\n                    \n                    with autocast(device_type='cuda'):\n                        preds = model(lr_batch)\n                        min_h = min(preds.shape[2], hr_batch.shape[2])\n                        min_w = min(preds.shape[3], hr_batch.shape[3])\n                        preds = preds[:, :, :min_h, :min_w]\n                        hr_batch = hr_batch[:, :, :min_h, :min_w]\n                        loss = criterion(preds, hr_batch)\n                    val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            \n            # Save best model\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_epoch = epoch\n                \n                # Save checkpoint\n                checkpoint = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'train_loss': avg_train_loss,\n                    'val_loss': avg_val_loss,\n                    'best_val_loss': best_val_loss\n                }\n                torch.save(checkpoint, 'best_dsstsr_model_fixed.pth')\n                torch.save(model.state_dict(), 'best_dsstsr_weights.pth')  # Weights only\n\n            # Check early stopping\n            if early_stopping(avg_val_loss, model):\n                print(f\"\\nðŸ›‘ Early stopping triggered at epoch {epoch}\")\n                print(f\"   Best validation loss: {early_stopping.best_loss:.6f}\")\n                print(f\"   No improvement for {early_stopping.patience * validation_frequency} epochs\")\n                print(f\"   Training stopped {num_epochs - epoch} epochs early\")\n                break\n\n        scheduler.step()\n\n        # Clean logging every 5 epochs\n        if epoch % 5 == 0:\n            epoch_time = time.time() - epoch_start_time\n            recent_losses = batch_losses[-10:] if len(batch_losses) >= 10 else batch_losses\n            avg_recent_loss = np.mean(recent_losses)\n            \n            print(f\"\\n[Epoch {epoch:3d}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Recent Batch Loss: {avg_recent_loss:.4f}\")\n            print(f\"                     Time: {epoch_time:.1f}s | LR: {scheduler.get_last_lr()[0]:.2e}\")\n            \n            if epoch % validation_frequency == 0:\n                print(f\"                     Val Loss: {avg_val_loss:.4f} | Best: {best_val_loss:.4f} (Epoch {best_epoch})\")\n                print(f\"                     Early Stop Counter: {early_stopping.counter}/{early_stopping.patience}\")\n            \n            # GPU memory info\n            if torch.cuda.is_available():\n                memory_used = torch.cuda.memory_allocated(device) / 1024**3\n                memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n                print(f\"                     GPU Memory: {memory_used:.1f}/{memory_total:.1f} GB ({memory_used/memory_total*100:.1f}%)\")\n\n    else:\n        print(f\"\\nâœ… Training completed all {num_epochs} epochs without early stopping\")\n        print(f\"   Best validation loss: {best_val_loss:.6f} at epoch {best_epoch}\")\n\n    # Save final checkpoint\n    final_checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'train_loss': avg_train_loss,\n        'best_val_loss': best_val_loss,\n        'final': True\n    }\n    torch.save(final_checkpoint, 'final_dsstsr_checkpoint.pth')\n\n    print(\"\\n--- Training Complete. Starting evaluation... ---\")\n    \n    # Find the best available model file\n    try:\n        model_path = find_best_model_file()\n    except FileNotFoundError as e:\n        print(f\"âŒ {e}\")\n        return None, None\n\n    print(f\"\\n--- Starting evaluation on {len(test_files)} test scenes ---\")\n    print(\"Preparing scene data for parallel processing...\")\n    scenes_data = []\n    hr_test_scenes = []\n    \n    for test_file_path in test_files:\n        hr_test_scene, lr_test_denoised_scene = prepare_data_cubes_from_folders([test_file_path], scale_factor=scale_factor)\n        if hr_test_scene.ndim == 4: hr_test_scene = hr_test_scene[0]\n        if lr_test_denoised_scene.ndim == 4: lr_test_denoised_scene = lr_test_denoised_scene[0]\n        \n        scene_name = os.path.basename(test_file_path)\n        scenes_data.append((lr_test_denoised_scene, hr_test_scene.shape, scene_name))\n        hr_test_scenes.append((scene_name, hr_test_scene))\n    \n    print(f\"Prepared {len(scenes_data)} scenes for parallel processing\")\n    \n    # Run parallel processing across GPUs\n    results = parallel_scene_processing(scenes_data, model_path, scale_factor)\n    \n    # Compute metrics\n    print(\"Computing evaluation metrics...\")\n    all_psnr, all_ssim, all_sam = [], [], []\n    hr_scenes_dict = {name: scene for name, scene in hr_test_scenes}\n    \n    for scene_name, reconstructed_hr_cube in results.items():\n        hr_original = hr_scenes_dict[scene_name]\n        print(f\"    Computing metrics for {scene_name}...\")\n        scene_metrics = compute_metrics(hr_original, reconstructed_hr_cube)\n        all_psnr.append(scene_metrics['PSNR'])\n        all_ssim.append(scene_metrics['SSIM'])\n        all_sam.append(scene_metrics['SAM'])\n        print(f\"    âœ… {scene_name} - PSNR: {scene_metrics['PSNR']:.2f}, SSIM: {scene_metrics['SSIM']:.4f}, SAM: {scene_metrics['SAM']:.2f}\")\n\n    final_metrics = {\n        'PSNR': np.mean(all_psnr),\n        'SSIM': np.mean(all_ssim),\n        'SAM': np.mean(all_sam)\n    }\n\n    print(f\"\\nðŸŽ‰ Evaluation Complete!\")\n    return final_metrics, reconstructed_hr_cube\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.246623Z","iopub.execute_input":"2025-08-05T08:14:36.246880Z","iopub.status.idle":"2025-08-05T08:14:36.489345Z","shell.execute_reply.started":"2025-08-05T08:14:36.246854Z","shell.execute_reply":"2025-08-05T08:14:36.488536Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\ndataset_path = '/kaggle/input/cave-hsi/'\n\nprint(f\"--- Attempting to list contents of: {dataset_path} ---\")\ntry:\n    top_level_contents = os.listdir(dataset_path)\n    print(\"Top-level contents are:\")\n    print(top_level_contents)\n\n    if top_level_contents:\n        # Check the first item in the list\n        first_item_name = top_level_contents[0]\n        first_item_path = os.path.join(dataset_path, first_item_name)\n        \n        if os.path.isdir(first_item_path):\n            print(f\"\\n--- Contents of the first folder '{first_item_name}' ---\")\n            print(os.listdir(first_item_path))\n        else:\n            print(f\"\\n'{first_item_name}' is a file, not a folder.\")\n\nexcept FileNotFoundError:\n    print(f\"âŒ CRITICAL ERROR: The path '{dataset_path}' does not exist. Please check your dataset name.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.490335Z","iopub.execute_input":"2025-08-05T08:14:36.490591Z","iopub.status.idle":"2025-08-05T08:14:36.518220Z","shell.execute_reply.started":"2025-08-05T08:14:36.490573Z","shell.execute_reply":"2025-08-05T08:14:36.517558Z"}},"outputs":[{"name":"stdout","text":"--- Attempting to list contents of: /kaggle/input/cave-hsi/ ---\nTop-level contents are:\n['oil_painting_ms', 'superballs_ms', 'egyptian_statue_ms', 'fake_and_real_tomatoes_ms', 'photo_and_face_ms', 'glass_tiles_ms', 'beads_ms', 'fake_and_real_lemon_slices_ms', 'hairs_ms', 'chart_and_stuffed_toy_ms', 'watercolors_ms', 'clay_ms', 'stuffed_toys_ms', 'fake_and_real_peppers_ms', 'fake_and_real_strawberries_ms', 'sponges_ms', 'face_ms', 'cd_ms', 'fake_and_real_beers_ms', 'real_and_fake_apples_ms', 'feathers_ms', 'fake_and_real_food_ms', 'jelly_beans_ms', 'balloons_ms', 'thread_spools_ms', 'flowers_ms', 'paints_ms', 'pompoms_ms', 'fake_and_real_sushi_ms', 'cloth_ms', 'real_and_fake_peppers_ms', 'fake_and_real_lemons_ms']\n\n--- Contents of the first folder 'oil_painting_ms' ---\n['oil_painting_ms']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\n\n# --- 1. Define File Path and Get List of All Valid Scene Folders ---\ndataset_path = '/kaggle/input/cave-hsi'\nall_scene_folders = []\n\nfor scene_name in os.listdir(dataset_path):\n    outer_path = os.path.join(dataset_path, scene_name)\n    inner_path = os.path.join(outer_path, scene_name)\n\n    if os.path.isdir(inner_path):\n        png_files = [f for f in os.listdir(inner_path) if f.endswith('.png') and '_ms_' in f]\n        if len(png_files) == 31:\n            all_scene_folders.append(inner_path)\n\n# Set random seed for reproducible splits\nrandom.seed(42)\nrandom.shuffle(all_scene_folders)\n\n# Split into 20 train, 6 validation, 6 test\ntrain_folders = all_scene_folders[:20]\nval_folders = all_scene_folders[20:26]  # 6 validation scenes\ntest_folders = all_scene_folders[26:32]  # 6 test scenes\n\nprint(f\"ðŸ“ Dataset split: {len(train_folders)} training, {len(val_folders)} validation, {len(test_folders)} test scenes\")\nprint(f\"   Total scenes used: {len(train_folders) + len(val_folders) + len(test_folders)} out of {len(all_scene_folders)} available\")\n\n# --- 2. Prepare Data Cubes ---\nprint(\"\\nðŸ”§ Preparing Training Data\")\nhr_train_cube, lr_train_denoised_cube = prepare_data_cubes_from_folders(train_folders)\nprint(f\"âœ… Final training HR shape: {hr_train_cube.shape}, LR shape: {lr_train_denoised_cube.shape}\")\nimport os\nfor file in ['best_dsstsr_weights.pth', 'best_dsstsr_model_fixed.pth', 'final_dsstsr_checkpoint.pth']:\n    if os.path.exists(file):\n        os.remove(file)\n# --- 3. Run Super-Resolution Experiment ---\nprint(\"\\nðŸš€ Running Super-Resolution (Optimized DSSTSR Model)\")\nsr_metrics, reconstructed_hr_cube = run_experiment_fixed(\n    hr_train_cube=hr_train_cube,\n    lr_train_denoised_cube=lr_train_denoised_cube,\n    test_files=test_folders,# Combine validation and test folders\n    val_folders=val_folders,\n    num_epochs=175,\n    hr_patch_size=63\n)\n\n# --- 4. Print SR Results ---\nprint(\"\\nðŸ“Š âœ… Final Super-Resolution Results:\")\nfor metric, value in sr_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(f\"\\nðŸ” Training completed on {len(train_folders)} scenes\")\nprint(f\"ðŸ” Validation used first scene from validation set during training\")\nprint(f\"ðŸ” Final evaluation on {len(val_folders) + len(test_folders)} scenes total\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T08:14:36.518868Z","iopub.execute_input":"2025-08-05T08:14:36.519063Z"}},"outputs":[{"name":"stdout","text":"ðŸ“ Dataset split: 20 training, 6 validation, 6 test scenes\n   Total scenes used: 32 out of 32 available\n\nðŸ”§ Preparing Training Data\n  Processing: paints_ms\n  Processing: glass_tiles_ms\n  Processing: watercolors_ms\n  Processing: sponges_ms\n  Processing: flowers_ms\n  Processing: clay_ms\n  Processing: jelly_beans_ms\n  Processing: beads_ms\n  Processing: real_and_fake_apples_ms\n  Processing: stuffed_toys_ms\n  Processing: face_ms\n  Processing: chart_and_stuffed_toy_ms\n  Processing: fake_and_real_sushi_ms\n  Processing: fake_and_real_strawberries_ms\n  Processing: thread_spools_ms\n  Processing: feathers_ms\n  Processing: real_and_fake_peppers_ms\n  Processing: superballs_ms\n  Processing: fake_and_real_peppers_ms\n  Processing: fake_and_real_beers_ms\nâœ… Final training HR shape: (20, 512, 512, 31), LR shape: (20, 512, 512, 31)\n\nðŸš€ Running Super-Resolution (Optimized DSSTSR Model)\n\n--- Starting Super-Resolution Experiment (Optimized Version) ---\nUsing provided validation folders...\n  Processing: egyptian_statue_ms\nActivating data parallelism on 2 GPUs!\nStarting training with early stopping...\nEarly stopping: patience=15, validation every 5 epochs\n\n[Epoch   5/175] Train Loss: 17.9207 | Recent Batch Loss: 16.6628\n                     Time: 261.8s | LR: 6.00e-05\n                     Val Loss: 15.3776 | Best: 15.3776 (Epoch 5)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  10/175] Train Loss: 10.0341 | Recent Batch Loss: 9.6174\n                     Time: 260.3s | LR: 1.00e-04\n                     Val Loss: 9.1329 | Best: 9.1329 (Epoch 10)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  15/175] Train Loss: 6.4303 | Recent Batch Loss: 6.2738\n                     Time: 260.6s | LR: 1.00e-04\n                     Val Loss: 6.0496 | Best: 6.0496 (Epoch 15)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  20/175] Train Loss: 4.4750 | Recent Batch Loss: 4.2223\n                     Time: 260.6s | LR: 1.00e-04\n                     Val Loss: 4.2075 | Best: 4.2075 (Epoch 20)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  25/175] Train Loss: 3.2918 | Recent Batch Loss: 3.1499\n                     Time: 259.9s | LR: 1.00e-04\n                     Val Loss: 3.0869 | Best: 3.0869 (Epoch 25)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  30/175] Train Loss: 2.4980 | Recent Batch Loss: 2.5210\n                     Time: 260.7s | LR: 1.00e-04\n                     Val Loss: 2.3265 | Best: 2.3265 (Epoch 30)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  35/175] Train Loss: 1.9419 | Recent Batch Loss: 1.8688\n                     Time: 261.2s | LR: 5.00e-05\n                     Val Loss: 1.9632 | Best: 1.9632 (Epoch 35)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  40/175] Train Loss: 1.5670 | Recent Batch Loss: 1.5469\n                     Time: 261.1s | LR: 5.00e-05\n                     Val Loss: 1.5380 | Best: 1.5380 (Epoch 40)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  45/175] Train Loss: 1.4077 | Recent Batch Loss: 1.4037\n                     Time: 259.9s | LR: 5.00e-05\n                     Val Loss: 1.3671 | Best: 1.3671 (Epoch 45)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  50/175] Train Loss: 1.2429 | Recent Batch Loss: 1.2596\n                     Time: 259.1s | LR: 5.00e-05\n                     Val Loss: 1.2443 | Best: 1.2443 (Epoch 50)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  55/175] Train Loss: 1.1037 | Recent Batch Loss: 1.0906\n                     Time: 260.7s | LR: 5.00e-05\n                     Val Loss: 1.0577 | Best: 1.0577 (Epoch 55)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  60/175] Train Loss: 0.9681 | Recent Batch Loss: 0.9578\n                     Time: 260.4s | LR: 2.50e-05\n                     Val Loss: 0.9379 | Best: 0.9379 (Epoch 60)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  65/175] Train Loss: 0.8544 | Recent Batch Loss: 0.8614\n                     Time: 261.2s | LR: 2.50e-05\n                     Val Loss: 0.8409 | Best: 0.8409 (Epoch 65)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  70/175] Train Loss: 0.8030 | Recent Batch Loss: 0.7838\n                     Time: 260.4s | LR: 2.50e-05\n                     Val Loss: 0.7812 | Best: 0.7812 (Epoch 70)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  75/175] Train Loss: 0.7458 | Recent Batch Loss: 0.7482\n                     Time: 260.9s | LR: 2.50e-05\n                     Val Loss: 0.7416 | Best: 0.7416 (Epoch 75)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  80/175] Train Loss: 0.6993 | Recent Batch Loss: 0.6895\n                     Time: 260.9s | LR: 2.50e-05\n                     Val Loss: 0.6828 | Best: 0.6828 (Epoch 80)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  85/175] Train Loss: 0.6439 | Recent Batch Loss: 0.6347\n                     Time: 263.0s | LR: 1.25e-05\n                     Val Loss: 0.6183 | Best: 0.6183 (Epoch 85)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  90/175] Train Loss: 0.6056 | Recent Batch Loss: 0.6078\n                     Time: 262.7s | LR: 1.25e-05\n                     Val Loss: 0.5758 | Best: 0.5758 (Epoch 90)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch  95/175] Train Loss: 0.5808 | Recent Batch Loss: 0.5760\n                     Time: 263.0s | LR: 1.25e-05\n                     Val Loss: 0.5489 | Best: 0.5489 (Epoch 95)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 100/175] Train Loss: 0.5547 | Recent Batch Loss: 0.5548\n                     Time: 261.1s | LR: 1.25e-05\n                     Val Loss: 0.5291 | Best: 0.5291 (Epoch 100)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 105/175] Train Loss: 0.5310 | Recent Batch Loss: 0.5426\n                     Time: 260.3s | LR: 1.25e-05\n                     Val Loss: 0.5096 | Best: 0.5096 (Epoch 105)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 110/175] Train Loss: 0.5079 | Recent Batch Loss: 0.5136\n                     Time: 259.6s | LR: 6.25e-06\n                     Val Loss: 0.4779 | Best: 0.4779 (Epoch 110)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 115/175] Train Loss: 0.4906 | Recent Batch Loss: 0.4817\n                     Time: 263.1s | LR: 6.25e-06\n                     Val Loss: 0.4551 | Best: 0.4551 (Epoch 115)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 120/175] Train Loss: 0.4820 | Recent Batch Loss: 0.4821\n                     Time: 263.7s | LR: 6.25e-06\n                     Val Loss: 0.4479 | Best: 0.4479 (Epoch 120)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 125/175] Train Loss: 0.4686 | Recent Batch Loss: 0.4698\n                     Time: 270.5s | LR: 6.25e-06\n                     Val Loss: 0.4452 | Best: 0.4452 (Epoch 125)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 130/175] Train Loss: 0.4594 | Recent Batch Loss: 0.4610\n                     Time: 261.2s | LR: 6.25e-06\n                     Val Loss: 0.4282 | Best: 0.4282 (Epoch 130)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 135/175] Train Loss: 0.4484 | Recent Batch Loss: 0.4534\n                     Time: 259.0s | LR: 3.13e-06\n                     Val Loss: 0.4242 | Best: 0.4242 (Epoch 135)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n\n[Epoch 140/175] Train Loss: 0.4423 | Recent Batch Loss: 0.4426\n                     Time: 262.9s | LR: 3.13e-06\n                     Val Loss: 0.4133 | Best: 0.4133 (Epoch 140)\n                     Early Stop Counter: 0/15\n                     GPU Memory: 0.1/14.7 GB (0.4%)\n","output_type":"stream"}],"execution_count":null}]}